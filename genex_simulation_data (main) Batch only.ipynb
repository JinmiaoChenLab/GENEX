{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6ba957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import random\n",
    "from numpy.random import default_rng\n",
    "from annoy import AnnoyIndex\n",
    "import torch.autograd as autograd\n",
    "from typing import List\n",
    "import anndata\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.metrics import (adjusted_rand_score, calinski_harabasz_score,\n",
    "                             normalized_mutual_info_score, silhouette_score)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "import utils\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "import colorcet as cc\n",
    "\n",
    "import math\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from random import sample\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.utils.spectral_norm as spectral_norm\n",
    "import torch.nn.utils.weight_norm as weight_norm\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import scanpy as sc\n",
    "import time\n",
    "import os\n",
    "from scipy import sparse\n",
    "from adabelief_pytorch import AdaBelief\n",
    "import torch.quantization\n",
    "\n",
    "import gzip\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "from utils.explanation_utils import explanation_hook, get_explanation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import gc\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = False # change to True for reproducible\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from anndata import AnnData\n",
    "    \n",
    "plt.ion()\n",
    "plt.show()\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "path= \"../\"\n",
    "\n",
    "# check available files\n",
    "# !ls ../real_data\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.autograd.profiler.profile(False)\n",
    "torch.autograd.profiler.emit_nvtx(False)\n",
    "\n",
    "########## NEURAL NETWORK UTILITY ##########\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "CPUTensor =  torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d45003de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## /acrc/jinmiao/CJM_lab/hoatran/demo_normalization/dataset/dataset5_human_pbmc/raw_data_python/myTotalData.h5ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e9b2333",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"simul1_dropout_47_b1_1000_b2_2000\"\n",
    "cellinfo_df = pd.read_csv('%s/cellinfo.txt' % (dataset), sep=\"\\t\")\n",
    "counts_df = pd.read_csv('%s/counts.txt' % (dataset), sep=\"\\t\")\n",
    "de_genes_df = pd.read_csv('%s/de_genes.txt' % (dataset), sep=\"\\t\")\n",
    "geneinfo_df = pd.read_csv('%s/geneinfo.txt' % (dataset), sep=\"\\t\")\n",
    "true_down_genes_df = pd.read_csv('%s/true_down_genes.txt' % (dataset), sep=\"\\t\")\n",
    "true_up_genes_df = pd.read_csv('%s/true_up_genes.txt' % (dataset), sep=\"\\t\")\n",
    "true_counts = pd.read_csv('%s/truecounts.txt' % (dataset), sep=\"\\t\").values.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdcaf5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 5000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "356f48fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 8000 × 5000\n",
       "    obs: 'Batch', 'Group', 'ExpLibSize'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from anndata import AnnData\n",
    "\n",
    "adata = AnnData(X=counts_df.transpose(), obs=cellinfo_df.iloc[:, 1:],\n",
    "                    dtype='float32')\n",
    "\n",
    "# obs_label_colname =\"celltype\"\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efdd5218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_counts = true_counts.astype('float')\n",
    "\n",
    "# true_counts = sc.pp.normalize_per_cell(true_counts, counts_per_cell_after=1e4, copy = True)\n",
    "# true_counts = np.log1p(true_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b019bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata.obsm[\"X_pca_true\"] = sc.pp.pca(true_counts, copy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96852147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.pp.neighbors(adata, use_rep='X_pca_true', n_neighbors=30)\n",
    "# # sc.external.pp.bbknn(adata_all, batch_key='batch', use_rep='X_latent')\n",
    "# sc.tl.umap(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b8e02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adata.obsm[\"X_latent\"] = output_results\n",
    "\n",
    "# # sc.pp.neighbors(adata, use_rep='X_latent', n_neighbors=30)\n",
    "# # sc.external.pp.bbknn(adata, batch_key='batch', use_rep='X_latent')\n",
    "# # sc.tl.umap(adata)\n",
    "# sc.pl.umap(adata, color=[\"Batch\",\"Group\"],\n",
    "#            palette=sc.pl.palettes.vega_20_scanpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "173dc20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "new_batch = le.fit_transform(adata.obs[\"Batch\"])\n",
    "adata.obs[\"Batch\"] = new_batch\n",
    "\n",
    "new_batch = [str(each) for each in new_batch]\n",
    "adata.obs[\"Batch_num\"] = new_batch\n",
    "\n",
    "# change also for the groups we want to preserve\n",
    "le_2 = preprocessing.LabelEncoder()\n",
    "new_groups = le_2.fit_transform(adata.obs[\"Group\"])\n",
    "new_groups = [int(each) for each in new_groups]\n",
    "# new_groups = [str(each) for each in new_groups]\n",
    "adata.obs[\"Group\"] = new_groups\n",
    "\n",
    "adata.obs[\"Group_num\"] = le_2.inverse_transform(adata.obs['Group'])\n",
    "adata.obs[\"Group_num\"] = [str(each) for each in adata.obs[\"Group_num\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bbe838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.layers[\"counts\"] = adata.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3735266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create dataset based on the text file\n",
    "\n",
    "# b1_obs = pd.read_csv(\"dataset5/b1_celltype.txt\", sep = \"\\t\", index_col = 0)\n",
    "# b2_obs = pd.read_csv(\"dataset5/b2_celltype.txt\", sep = \"\\t\", index_col = 0)\n",
    "\n",
    "# b1_expr = pd.read_csv(\"dataset5/b1_exprs.txt\", sep = \"\\t\", index_col = 0).values.transpose()\n",
    "# b2_expr = pd.read_csv(\"dataset5/b2_exprs.txt\", sep = \"\\t\", index_col = 0).values.transpose()\n",
    "\n",
    "# b1_obs[\"batch\"] = \"0\"\n",
    "# b2_obs[\"batch\"] = \"1\"\n",
    "\n",
    "# full_expr = np.concatenate((b1_expr, b2_expr), axis=0)\n",
    "# full_obs = pd.concat([b1_obs, b2_obs])\n",
    "\n",
    "# adata = AnnData(X=full_expr, obs=full_obs, dtype='float32')\n",
    "\n",
    "# adata.write_h5ad(\"data/pbmc.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5bfd4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "\n",
    "\n",
    "def ari(adata, group1, group2, implementation=\"sklearn\"):\n",
    "    \"\"\"Adjusted Rand Index\n",
    "    The function is symmetric, so group1 and group2 can be switched\n",
    "    For single cell integration evaluation the comparison is between predicted cluster\n",
    "    assignments and the ground truth (e.g. cell type)\n",
    "    :param adata: anndata object\n",
    "    :param group1: string of column in adata.obs containing labels\n",
    "    :param group2: string of column in adata.obs containing labels\n",
    "    :param implementation: if set to 'sklearn', uses sklearn's implementation,\n",
    "        otherwise native implementation is taken\n",
    "    \"\"\"\n",
    "\n",
    "    group1 = adata.obs[group1].to_numpy()\n",
    "    group2 = adata.obs[group2].to_numpy()\n",
    "\n",
    "    if len(group1) != len(group2):\n",
    "        raise ValueError(\n",
    "            f\"different lengths in group1 ({len(group1)}) and group2 ({len(group2)})\"\n",
    "        )\n",
    "\n",
    "    return adjusted_rand_score(group1, group2)\n",
    "\n",
    "def compute_ari(adata):\n",
    "\n",
    "    resolutions = None\n",
    "\n",
    "    if resolutions is None:\n",
    "        n = 20\n",
    "        resolutions = [2 * x / n for x in range(1, n + 1)]\n",
    "\n",
    "    score_max = 0\n",
    "    res_max = resolutions[0]\n",
    "    clustering = None\n",
    "    score_all = []\n",
    "    use_rep = \"X_pca\"\n",
    "    cluster_key = \"leiden\"\n",
    "    label_key = \"Group\"\n",
    "    sc.pp.neighbors(adata, use_rep=use_rep)\n",
    "\n",
    "    for res in resolutions:\n",
    "        sc.tl.leiden(adata, resolution=res, key_added=cluster_key)\n",
    "        score = ari(adata, label_key, cluster_key)\n",
    "        score_all.append(score)\n",
    "        if score_max < score:\n",
    "            score_max = score\n",
    "            res_max = res\n",
    "            clustering = adata.obs[cluster_key]\n",
    "\n",
    "    return score_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "167c8dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.3'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61404d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flavor: seurat_v3, cell_ranger & log = false\n",
    "def sub_data_preprocess(adata: sc.AnnData, n_top_genes: int = 5000, batch_key: str = None, flavor: str = 'seurat_v3', min_genes: int = 200, min_cells: int = 3) -> sc.AnnData:\n",
    "    sc.pp.filter_cells(adata, min_genes=min_genes)\n",
    "    sc.pp.filter_genes(adata, min_cells=min_cells)\n",
    "#     if flavor == 'seurat_v3':\n",
    "# # #         count data is expected when flavor=='seurat_v3'\n",
    "# #         sc.pp.highly_variable_genes(\n",
    "# #             adata, flavor=flavor, batch_key = batch_key)\n",
    "#         sc.pp.highly_variable_genes(\n",
    "#             adata, flavor=flavor, batch_key = batch_key, n_top_genes=n_top_genes)\n",
    "\n",
    "#     if flavor != 'seurat_v3':.\n",
    "#         # log-format data is expected when flavor!='seurat_v3'\n",
    "#         sc.pp.highly_variable_genes(\n",
    "#             adata, n_top_genes=n_top_genes, flavor=flavor)\n",
    "    sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    return adata\n",
    "\n",
    "\n",
    "def data_preprocess(adata: sc.AnnData, key: str = 'batch', n_top_genes: int = 10000, flavor: str = 'seurat_v3', min_genes: int = 200, min_cells: int = 3, n_batch: int = 2) -> sc.AnnData:\n",
    "    print('Establishing Adata for Next Step...')\n",
    "    hv_adata = sub_data_preprocess(adata, n_top_genes=n_top_genes, batch_key = key, flavor=flavor, min_genes=min_genes, min_cells=min_cells)\n",
    "#     if len(adata.var.index) > n_top_genes:\n",
    "#         hv_adata = hv_adata[:, hv_adata.var['highly_variable']]\n",
    "#     hv_adata.X = np.expm1(hv_adata.X)\n",
    "    print('PreProcess Done.')\n",
    "    return hv_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a08dcccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Establishing Adata for Next Step...\n",
      "PreProcess Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 7891 × 4695\n",
       "    obs: 'Batch', 'Group', 'ExpLibSize', 'Batch_num', 'Group_num', 'n_genes', 'n_counts'\n",
       "    var: 'n_cells'\n",
       "    uns: 'log1p'\n",
       "    layers: 'counts'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_str = \"Batch_num\"\n",
    "\n",
    "adata = data_preprocess(adata, batch_str)\n",
    "adata  # Output the basic information of the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4615ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.layers[\"log_norm\"] = adata.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "219a3a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc05189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scanpy as sc\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "\n",
    "# # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "# sc.settings.verbosity = 1\n",
    "# sc.settings.set_figure_params(\n",
    "#     dpi=300, frameon=False, figsize=(3, 3), facecolor='white')\n",
    "\n",
    "# # check the umap after normalisation\n",
    "\n",
    "# # compute pca and neighbor for calculating the ARI score\n",
    "# # sc.pp.scale(adata)\n",
    "# sc.pp.pca(adata)\n",
    "# sc.pp.neighbors(adata, n_neighbors=30)\n",
    "# sc.tl.umap(adata)\n",
    "\n",
    "# # sc.pl.umap(adata, color=['batch', 'CellType'],\n",
    "# #            palette=sc.pl.palettes.vega_20_scanpy)\n",
    "\n",
    "# # revert based to unscale data\n",
    "# # adata.X = adata.layers[\"log_norm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d527d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.pl.umap(adata, color=['Batch_num'],\n",
    "#            palette=sc.pl.palettes.vega_20_scanpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76b5573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.pl.umap(adata, color=['Group_num'],\n",
    "#            palette=sc.pl.palettes.vega_20_scanpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af429d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"simul111_dropout_1_b1_1000_b2_2000\"\n",
    "\n",
    "# # true_down_genes_df = pd.read_csv('%s/true_down_genes.txt' % (dataset), sep=\"\\t\")\n",
    "# # true_up_genes_df = pd.read_csv('%s/true_up_genes.txt' % (dataset), sep=\"\\t\")\n",
    "# true_counts = pd.read_csv('%s/counts.txt' % (dataset), sep=\"\\t\").values.transpose()\n",
    "\n",
    "# true_counts = true_counts.astype('float')\n",
    "\n",
    "# true_counts = sc.pp.normalize_per_cell(true_counts, counts_per_cell_after=1e4, copy = True)\n",
    "# true_counts = np.log1p(true_counts)\n",
    "\n",
    "# subset_index = [int(each[4:]) - 1 for each in adata.obs.index.values]\n",
    "# subset_genes = [int(each[4:]) - 1 for each in adata.var.index.values]\n",
    "# true_counts = true_counts[subset_index]\n",
    "# true_counts = true_counts[:, subset_genes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fcef19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata.obsm[\"X_pca_true\"] = sc.pp.pca(true_counts, copy = True)\n",
    "\n",
    "# sc.pp.neighbors(adata, use_rep='X_pca_true', n_neighbors=30)\n",
    "# # sc.external.pp.bbknn(adata_all, batch_key='batch', use_rep='X_latent')\n",
    "# sc.tl.umap(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8a96df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adata.obsm[\"X_latent\"] = output_results\n",
    "\n",
    "# # sc.pp.neighbors(adata, use_rep='X_latent', n_neighbors=30)\n",
    "# # sc.external.pp.bbknn(adata, batch_key='batch', use_rep='X_latent')\n",
    "# # sc.tl.umap(adata)\n",
    "# sc.pl.umap(adata, color=[\"Batch_num\",\"Group_num\"],\n",
    "#            palette=sc.pl.palettes.vega_20_scanpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a38b4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata.layers[\"X_raw\"] = adata.X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68a873c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_array = adata.layers[\"X_raw\"]\n",
    "\n",
    "# # Flatten the 2D array to 1D\n",
    "# flattened_array = raw_array.flatten()\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# a = np.amin(raw_array)\n",
    "# b = np.amax(raw_array)\n",
    "# ax.hist(flattened_array, bins=np.arange(a, b, (b-a)*0.03), alpha=1.0, rwidth=0.90, color='salmon')\n",
    "\n",
    "# # generate the percentage of zeroes and negative values in the array\n",
    "# percent_zeros = (raw_array == 0).mean()\n",
    "# percent_negatives = (raw_array < 0).mean()\n",
    "\n",
    "# # Remove the top and right spines\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# ax.spines['right'].set_visible(False)\n",
    "\n",
    "# # Add labels and title to the plot\n",
    "# ax.set_xlabel('Values')\n",
    "# ax.set_ylabel('Log Frequency')\n",
    "# plt.yscale('log')\n",
    "# # plt.title(\"Histogram of Gene Expression\")\n",
    "\n",
    "# plt.annotate(\"Zeros values: {:.2f}%\\nNegatives values: {:.2f}%\".format(percent_zeros * 100, percent_negatives * 100),\n",
    "#              xy=(0.98, 0.98), xycoords=\"axes fraction\", fontsize=10, ha=\"right\", va=\"top\",\n",
    "#              bbox=dict(boxstyle=\"round\", facecolor=\"mistyrose\", alpha=0.5))\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c3a3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the colorcet paletteasdasd\n",
    "# custom_palette = cc.glasbey_category10\n",
    "\n",
    "# ax = sc.pl.umap(adata, color='Method', palette=custom_palette,\n",
    "#            frameon=False, legend_fontsize=3.5, show=False)\n",
    "\n",
    "# # Set the plot title with the desired font size\n",
    "# ax.set_title('Predicted Cell Type', fontsize=6)\n",
    "\n",
    "# # Adjust the legend font size\n",
    "# handles, labels = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "# # Adjust the size of the circles\n",
    "# for handle in handles:\n",
    "#     handle.set_sizes([12])\n",
    "#     handle.set_edgecolor('black')\n",
    "#     handle.set_linewidth(0.5)\n",
    "\n",
    "# # Move the legend to the right side of the plot\n",
    "# plt.legend(handles=handles, labels=labels, prop={'size': 5}, ncol = 1, \n",
    "#            loc='upper center', bbox_to_anchor=(0.5, -0.08))\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "571e65f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder(handle_unknown='ignore')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# le = preprocessing.LabelEncoder()\n",
    "enc_batch = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_group = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "label_str = \"Batch\"\n",
    "group_str = \"Group\"\n",
    "\n",
    "enc_batch.fit(adata.obs[label_str].to_numpy().reshape(-1, 1))\n",
    "enc_group.fit(adata.obs[group_str].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae29e84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CLASS SINGLE CELL DATASET ##########\n",
    "class ScDataset(Dataset):\n",
    "    def __init__(self, adata):\n",
    "        self.variable = None\n",
    "        self.labels = adata.obs[label_str].to_numpy()\n",
    "        self.org_batch = adata.obs[label_str].to_numpy()\n",
    "        self.groups = adata.obs[group_str].to_numpy()\n",
    "        self.one_hot_labels = enc_batch.transform(adata.obs[label_str].to_numpy().reshape(-1, 1)).toarray()\n",
    "        self.one_hot_groups = enc_group.transform(adata.obs[group_str].to_numpy().reshape(-1, 1)).toarray()\n",
    "        try:\n",
    "            self.X = adata.X.toarray()\n",
    "#             self.hvg_X = adata[:, adata.var['highly_variable']].X.copy().toarray()\n",
    "        except:\n",
    "            self.X = adata.X\n",
    "        \n",
    "        self.min_val = np.amin(adata.X)\n",
    "        self.max_val = np.amax(adata.X)\n",
    "#         self.groups = adata.obs[group_str].to_numpy()\n",
    "        self.transform = None\n",
    "        self.sample = None\n",
    "        self.adata = adata\n",
    "#         self.random_target = np.eye(c_dim)[np.random.choice(c_dim, len(self.labels))]\n",
    "        \n",
    "#             self.hvg_X = adata[:, adata.var['highly_variable']].X.copy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         random_index = random.randint(0, self.X.shape[0] - 1)\n",
    "\n",
    "        return self.X[index], self.labels[index], self.one_hot_labels[index], self.groups[index], self.one_hot_groups[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5f90829",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_str = \"Batch\"\n",
    "\n",
    "# split per batch into new objects.\n",
    "batches = list(set(adata.obs[label_str]))\n",
    "alldata = {}\n",
    "for batch in batches:\n",
    "    alldata[batch] = adata[adata.obs[label_str] == batch,]\n",
    "\n",
    "length_data = []\n",
    "\n",
    "for batch in batches:\n",
    "    length = len(alldata[batch])\n",
    "    \n",
    "    length_data.append(length)\n",
    "anchor_index = np.argmax(length_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d036d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[anchor_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b56040",
   "metadata": {},
   "source": [
    "### pre processing step for normalise and scale the for each individual batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cedf22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # apply the pre processing onto the anndata\n",
    "# # first create layer for the anndata\n",
    "\n",
    "adata.layers[\"X_raw\"] = adata.layers[\"log_norm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cc1fb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 1.1481607, 0.       ,\n",
       "        0.       ],\n",
       "       ...,\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e231b8ba",
   "metadata": {},
   "source": [
    "## compute ARI score for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55b4ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split per batch into new objects.\n",
    "# batches = list(set(adata.obs[label_str]))\n",
    "# alldata = {}\n",
    "# for batch in batches:\n",
    "#     alldata[batch] = adata[adata.obs[label_str] == batch,]\n",
    "    \n",
    "# # subset only the batches with good ARI score\n",
    "# subset_batches = []\n",
    "\n",
    "# # max_ari = 0\n",
    "    \n",
    "# for each in batches:\n",
    "#     print(\"batch: %s\" % each)\n",
    "#     ari_score = compute_ari(alldata[each])\n",
    "#     print(ari_score)\n",
    "#     if ari_score > 0.50:\n",
    "#         subset_batches.append(each)\n",
    "        \n",
    "# #     if ari_score > max_ari:\n",
    "# #         max_ari = ari_score\n",
    "# #         batch_anchor = each\n",
    "        \n",
    "\n",
    "# # then subset the adata for training the model\n",
    "# # save the test dataset first\n",
    "# adata = adata.copy()\n",
    "# adata_temp = adata[adata.obs[\"batch\"].isin(subset_batches)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9edfa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try try using the normalised data as the input\n",
    "\n",
    "# adata.X = adata.layers[\"X_norm\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3c78c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata.X = adata.X.toarray().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5134220a",
   "metadata": {},
   "source": [
    "## Normalise the sequencing depth which is the library size\n",
    "\n",
    "The the batch effect removal starting from removing the library size and then remove the actual batch effect from the technical different.\n",
    "\n",
    "**Previous issue**: we select one batch as the anchor which become and issue when that batch does not seperate cell type well enough as compare to other batch. Hence, we now change to batch free without selecting any anchored batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f374ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scanpy as sc\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "\n",
    "# # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "# sc.settings.verbosity = 1\n",
    "# sc.settings.set_figure_params(\n",
    "#     dpi=150, frameon=False, figsize=(3, 3), facecolor='white')\n",
    "\n",
    "# # adata.X = adata.layers[\"X_scaled\"].copy()\n",
    "# # check the umap after normalisation\n",
    "\n",
    "# sc.pp.pca(adata)\n",
    "# sc.pp.neighbors(adata)\n",
    "# sc.tl.umap(adata)\n",
    "\n",
    "# sc.pl.umap(adata, color=['batch', 'celltype'],\n",
    "#            palette=sc.pl.palettes.vega_20_scanpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afffc181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper bound of the gene expression:  7.118697\n",
      "Lower bound of the gene expression:  0.0\n"
     ]
    }
   ],
   "source": [
    "max_val = np.amax(adata.X)\n",
    "min_val = np.amin(adata.X)\n",
    "\n",
    "print(\"Upper bound of the gene expression: \", max_val)\n",
    "print(\"Lower bound of the gene expression: \", min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d41ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_div_gp(discriminator, real_samples, fake_samples):\n",
    "# # #     real_data = Variable(real_data, requires_grad=True)\n",
    "# #     prob_real_data, _, = D(real_data)\n",
    "\n",
    "#     batch_size = real_samples.size(0)\n",
    "\n",
    "#     # Generate random weights for interpolation\n",
    "#     alpha = torch.rand(batch_size, 1).to(real_samples.device)\n",
    "\n",
    "#     # Create interpolated samples\n",
    "#     interpolated = alpha * real_samples + (1 - alpha) * fake_samples\n",
    "#     interpolated = Variable(interpolated, requires_grad = True)\n",
    "\n",
    "#     # Calculate the discriminator's score for interpolated samples\n",
    "#     d_interpolated = discriminator(interpolated)[0]\n",
    "\n",
    "#     # Compute gradients of d_interpolated with respect to interpolated\n",
    "#     gradients = autograd.grad(\n",
    "#         outputs=d_interpolated,\n",
    "#         inputs=interpolated,\n",
    "#         grad_outputs=torch.ones(d_interpolated.size()).to(real_samples.device),\n",
    "#         create_graph=True,\n",
    "#         retain_graph=True,\n",
    "#         only_inputs=True\n",
    "#     )[0]\n",
    "\n",
    "#     # Compute the gradient penalty\n",
    "#     gradients = gradients.view(batch_size, -1)\n",
    "#     gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "#     return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ad6d0a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_div_gp(prob_real_data, prob_fake_data, real_data, fake_data, k=2, p=8):\n",
    "# #     real_data = Variable(real_data, requires_grad=True)\n",
    "#     prob_real_data, _, = D(real_data)\n",
    "\n",
    "    real_grad_outputs = torch.ones(prob_real_data.size()).cuda() if cuda else torch.ones(prob_real_data.size())\n",
    "\n",
    "    fake_data.requires_grad_(True)\n",
    "#     prob_fake_data, _, = D(fake_data)\n",
    "\n",
    "    fake_grad_outputs = torch.ones(prob_fake_data.size()).cuda() if cuda else torch.ones(prob_fake_data.size())\n",
    "\n",
    "    real_gradient = torch.autograd.grad(\n",
    "        outputs=prob_real_data,\n",
    "        inputs=real_data,\n",
    "        grad_outputs=real_grad_outputs,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    \n",
    "    fake_gradient = torch.autograd.grad(\n",
    "        outputs=prob_fake_data,\n",
    "        inputs=fake_data,\n",
    "        grad_outputs=fake_grad_outputs,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True, \n",
    "    )[0]\n",
    "\n",
    "    real_gradient_norm = real_gradient.view(real_gradient.size(0), -1).pow(2).sum(1) ** (p / 2)\n",
    "    fake_gradient_norm = fake_gradient.view(fake_gradient.size(0), -1).pow(2).sum(1) ** (p / 2)\n",
    "\n",
    "    gradient_penalty = torch.mean(real_gradient_norm + fake_gradient_norm) * 1.\n",
    "     \n",
    "    return gradient_penalty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c9b92b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RBF(nn.Module):\n",
    "#     def __init__(self, n_kernels=32, mul_factor=2.0, bandwidth=None):\n",
    "#         super().__init__()\n",
    "#         self.bandwidth_multipliers = mul_factor ** (torch.arange(n_kernels, device = \"cuda\") - n_kernels // 2)\n",
    "#         self.bandwidth = bandwidth\n",
    "\n",
    "#     def get_bandwidth(self, L2_distances):\n",
    "#         if self.bandwidth is None:\n",
    "#             n_samples = L2_distances.shape[0]\n",
    "#             return L2_distances.data.sum() / (n_samples ** 2 - n_samples)\n",
    "\n",
    "#         return self.bandwidth\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         L2_distances = torch.cdist(X, X) ** 2\n",
    "#         return torch.exp(-L2_distances[None, ...] / (self.get_bandwidth(L2_distances) * self.bandwidth_multipliers)[:, None, None]).sum(dim=0)\n",
    "\n",
    "# class MMDLoss(nn.Module):\n",
    "#     def __init__(self, kernel=RBF()):\n",
    "#         super().__init__()\n",
    "#         self.kernel = kernel\n",
    "\n",
    "#     def forward(self, X, Y):\n",
    "#         K = self.kernel(torch.vstack([X, Y]))\n",
    "\n",
    "#         X_size = X.shape[0]\n",
    "#         XX = K[:X_size, :X_size].mean()\n",
    "#         XY = K[:X_size, X_size:].mean()\n",
    "#         YY = K[X_size:, X_size:].mean()\n",
    "\n",
    "#         return XX - 2 * XY + YY\n",
    "\n",
    "# class CovarianceLoss(nn.Module):\n",
    "\n",
    "#     def forward(self, X, Y):\n",
    "#         X_cov = torch.cov(X.T)\n",
    "#         Y_cov = torch.cov(Y.T)\n",
    "#         return torch.norm(X_cov - Y_cov, p='fro')\n",
    "\n",
    "# class MMCDCovarianceLoss(nn.Module):\n",
    "\n",
    "#     def __init__(self, kernel=RBF()):\n",
    "#         super().__init__()\n",
    "#         self.kernel = kernel\n",
    "#         self.covariance_loss = CovarianceLoss()\n",
    "\n",
    "#     def forward(self, X, Y):\n",
    "#         loss = MMCDLoss(kernel=self.kernel)\n",
    "#         mmcd_loss = loss(X, Y)\n",
    "#         covariance_loss = self.covariance_loss(X, Y)\n",
    "#         return mmcd_loss + covariance_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "74858edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        torch.nn.init.kaiming_normal_(m.weight.data, 0.2)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "38e45865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFF(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(AFF, self).__init__()\n",
    "\n",
    "        # attentional feature fusion layer\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim, dim),\n",
    "           # nn.BatchNorm1d(dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, latent_data, style_latent):\n",
    "        # fusion layer before decoder\n",
    "        xa = latent_data + style_latent\n",
    "        xl = self.attention(xa)\n",
    "        wei = self.sigmoid(xl)\n",
    "\n",
    "        data = latent_data * wei + style_latent * ((1 - wei))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c6fba610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(a, b):\n",
    "    return torch.cat((a, b), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "88af0a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneExpressionAttention(nn.Module):\n",
    "    def __init__(self, n_features, d_k, bias = True):\n",
    "        super(GeneExpressionAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.bias = bias\n",
    "        \n",
    "        # query representation\n",
    "        self.W_Q = nn.Linear(n_features, d_k, self.bias)\n",
    "        \n",
    "        # key and value pair to attend to\n",
    "        self.W_K = nn.Linear(n_features, d_k, self.bias)\n",
    "        self.W_V = nn.Linear(n_features, d_k, self.bias)\n",
    "        \n",
    "        # multihead attention\n",
    "#         self.multi_atten = nn.MultiheadAttention(d_k, 4, kdim = n_features, vdim = n_features, batch_first = True)\n",
    "\n",
    "#         self.encoder = nn.GRU(n_features, d_k, num_layers = 1, bidirectional = False, batch_first = True)\n",
    "\n",
    "    def forward(self, tabular_data):\n",
    "        # Project tabular data into Q, K, and V matrices\n",
    "        Q = self.W_Q(tabular_data)\n",
    "        K = self.W_K(tabular_data)\n",
    "        V = self.W_V(tabular_data)\n",
    "        \n",
    "        Q, K, V = Q.unsqueeze(1), K.unsqueeze(1), V.unsqueeze(1)\n",
    "        \n",
    "#         attn_output, attn_output_weights = self.multi_atten(Q, K, V)\n",
    "        \n",
    "#         return attn_output.squeeze(1)\n",
    "\n",
    "        return F.scaled_dot_product_attention(Q,K,V, is_causal = True).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6994dc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableWeightedSum(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrainableWeightedSum, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.Tensor(2))  # Create learnable weights\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.uniform_(self.weights)  # Initialize weights with uniform distribution\n",
    "\n",
    "    def forward(self, tensor1, tensor2):\n",
    "        # Apply element-wise multiplication to the input tensors\n",
    "        weighted_tensor1 = torch.mul(tensor1, self.weights[0])\n",
    "        weighted_tensor2 = torch.mul(tensor2, self.weights[1])\n",
    "\n",
    "        # Sum the weighted tensors\n",
    "        fused_tensor = torch.add(weighted_tensor1, weighted_tensor2)\n",
    "\n",
    "        return fused_tensor\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias = True):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        \n",
    "        self.feed_forward  = nn.Sequential(\n",
    "            nn.Linear(in_features, in_features, bias),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features, in_features, bias),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.zeros(1))  # Create learnable weights\n",
    "        \n",
    "        self.atten = GeneExpressionAttention(in_features, in_features, bias)\n",
    "#         self.weighted_sum = TrainableWeightedSum()\n",
    "#         self.weighted_sum.cuda()\n",
    "\n",
    "        self.norm1 = nn.BatchNorm1d(in_features)\n",
    "        self.norm2 = nn.BatchNorm1d(out_features)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # output layer\n",
    "        \n",
    "        self.out = nn.Linear(in_features, out_features, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        atten_out = self.norm1(self.dropout(self.gamma * self.atten(x)) + x)\n",
    "#         atten_out_2 = self.atten2(x)\n",
    "        out = self.feed_forward(atten_out)\n",
    "        out = atten_out + self.dropout(out)\n",
    "        out = self.norm2(out)\n",
    "        return self.out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0869ff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sophia import SophiaG\n",
    "from utils.acprop import ACProp\n",
    "# from pytorch_optimizer import *\n",
    "from adabelief_pytorch import AdaBelief\n",
    "from pytorch_optimizer import clip_grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2861c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_X, min_val, max_val = minmax_scale(adata.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f1910441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata.X = scaled_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0017027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_dropout_rate = 0.8\n",
    "final_dropout_rate = 0.25\n",
    "stabilization_epochs = 200\n",
    "dropout_anneal_epochs = 500 - stabilization_epochs\n",
    "dropout_decay = (initial_dropout_rate - final_dropout_rate) / dropout_anneal_epochs\n",
    "\n",
    "# Function to anneal the dropout rate\n",
    "def anneal_dropout_rate(epoch):\n",
    "    if epoch < stabilization_epochs:\n",
    "        return initial_dropout_rate\n",
    "    else:\n",
    "        return max(initial_dropout_rate - dropout_decay * (epoch - stabilization_epochs), final_dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "99b710b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.parametrizations import orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "57ede6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalised(x, min_val, max_val):\n",
    "    return -1 + 2 * (x - min_val) / (max_val - min_val)\n",
    "\n",
    "def denormalised(x_normalized, min_val, max_val):\n",
    "    return min_val + (x_normalized + 1) * 0.5 * (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d581a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dog import DoG, LDoG\n",
    "from dog import PolynomialDecayAverager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3877a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchDiscriminationLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, minibatch_normal_init=True):\n",
    "        super(MinibatchDiscriminationLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.minibatch_normal_init = minibatch_normal_init\n",
    "        self.T = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        M = torch.mm(x, self.T)\n",
    "        if self.minibatch_normal_init:\n",
    "            M = M - torch.mean(M, dim=0, keepdim=True)\n",
    "        M = torch.abs(M)\n",
    "#         M = torch.sum(M, dim=0)  # Keepdim preserves the shape\n",
    "        print(M.shape)\n",
    "        print(x.shape)\n",
    "        return torch.cat([x, M], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "635c75a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(scd, n_epochs):\n",
    "#     n_epochs = n_epochss\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_size = scd.X.shape[1]\n",
    "    lr = 0.0001\n",
    "    b1 = 0.5\n",
    "    b2 = 0.999\n",
    "    latent_dim = 1024\n",
    "#     style_dim = latent_dim\n",
    "    embedding_dim = data_size\n",
    "    n_dim = 1024\n",
    "#     proj_dim = 2000\n",
    "    n_critic = 5\n",
    "    global c_dim, ctype_dim\n",
    "    c_dim = len(list(set(adata.obs[label_str])))\n",
    "    g_dim = len(list(set(adata.obs[group_str])))\n",
    "    \n",
    "    # early stopping function\n",
    "    trigger_times = 0\n",
    "    best_loss = 100.0\n",
    "    \n",
    "    # condition for when we don't have anything to preserve\n",
    "    con_dim = c_dim\n",
    "    \n",
    "    act_func = nn.ReLU(inplace = True)\n",
    "\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self, dropout_rate = 0.16, res_blocks = 3):\n",
    "                super(Generator, self).__init__()\n",
    "                self.relu = nn.ReLU(inplace = True)\n",
    "                self.dropout_rate = dropout_rate\n",
    "                \n",
    "                # encoder for the gene expression\n",
    "                self.encoder = nn.Sequential(\n",
    "#                     self.proj, # adding projection layer\n",
    "                    GeneExpressionAttention(data_size, n_dim),\n",
    "                    nn.BatchNorm1d(n_dim),\n",
    "                    nn.Dropout(self.dropout_rate),\n",
    "                    act_func,\n",
    "                    nn.Linear(n_dim, n_dim // 2),\n",
    "                    nn.BatchNorm1d(n_dim // 2),\n",
    "                    nn.Dropout(self.dropout_rate),\n",
    "                    act_func,\n",
    "                    nn.Linear(n_dim // 2, latent_dim),\n",
    "#                     nn.BatchNorm1d(latent_dim),\n",
    "#                     nn.Dropout(self.dropout_rate),\n",
    "#                     act_func,\n",
    "                )\n",
    "                \n",
    "                # create a share decoder\n",
    "                self.decoder = nn.Sequential(\n",
    "                    nn.Linear(latent_dim + embedding_dim, n_dim // 2),\n",
    "                    nn.BatchNorm1d(n_dim // 2),\n",
    "                    nn.Dropout(self.dropout_rate),\n",
    "                    act_func,\n",
    "                    nn.Linear(n_dim // 2, n_dim),\n",
    "                    nn.BatchNorm1d(n_dim),\n",
    "                    nn.Dropout(self.dropout_rate),\n",
    "                    act_func,\n",
    "                    nn.Linear(n_dim, data_size),\n",
    "#                     nn.Linear(n_dim, data_size),\n",
    "                )\n",
    "                \n",
    "                \n",
    "                self.embedding_layer_c = nn.Sequential(\n",
    "                    nn.Embedding(c_dim, embedding_dim),\n",
    "#                     nn.BatchNorm1d(embedding_dim)\n",
    "                )\n",
    "                \n",
    "#                 self.embedding_layer_g = nn.Sequential(\n",
    "#                     nn.Embedding(g_dim, embedding_dim),\n",
    "#                 )\n",
    " \n",
    "        def forward(self, x, c, test = False):\n",
    "        \n",
    "            batch_size = x.shape[0]\n",
    "            # when the model is trained, we discard the c label\n",
    "            if test == True:\n",
    "                # zero out to remove this attribute from the model\n",
    "                c_embeddings = torch.zeros(batch_size, embedding_dim, device = x.device)\n",
    "            else:\n",
    "                c_embeddings = self.embedding_layer_c(c)\n",
    "            \n",
    "#             g_embeddings = self.embedding_layer_g(g)\n",
    "#             data = torch.cat((x, c_embeddings), -1)\n",
    "            \n",
    "#             out_proj = self.proj(data)\n",
    "#             atten_out = self.atten_out(data) + self.random_atten_out(data)\n",
    "        \n",
    "            # content network\n",
    "            latent_data = self.encoder(x)\n",
    "#             latent_data = self.resnet(latent_data)\n",
    "            latent_data = torch.cat((latent_data, c_embeddings), -1)\n",
    "            corrected_data = self.decoder(latent_data)\n",
    "#             atten_corrected = self.decoder(atten_out)\n",
    "            \n",
    "            return corrected_data\n",
    "    \n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self, dropout_rate = 0.16):\n",
    "            super(Discriminator, self).__init__()\n",
    "            \n",
    "#             self.self_att = nn.Sequential(\n",
    "#                     GeneExpressionAttention(data_size, embedding_dim),\n",
    "#                     LinearAttention(embedding_dim, embedding_dim),\n",
    "#             )\n",
    "            \n",
    "            self.dropout_rate = dropout_rate\n",
    "            \n",
    "            self.model = nn.Sequential(\n",
    "                GeneExpressionAttention(data_size, n_dim), # adding projection layer\n",
    "                nn.BatchNorm1d(n_dim),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.LeakyReLU(0.2, inplace = True),\n",
    "                nn.Linear(n_dim, n_dim // 2),\n",
    "                nn.BatchNorm1d(n_dim // 2),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "                nn.LeakyReLU(0.2, inplace = True),\n",
    "                nn.Linear(n_dim // 2, latent_dim),\n",
    "                nn.BatchNorm1d(latent_dim),\n",
    "                nn.LeakyReLU(0.2, inplace = True),\n",
    "            )\n",
    "        \n",
    "            # Output layers\n",
    "            self.adv_layer = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(latent_dim, 1)\n",
    "                ) for _ in range(c_dim)\n",
    "            ])\n",
    "            \n",
    "#             self.adv_layer = nn.Sequential(\n",
    "#                 nn.Linear(latent_dim, 1),\n",
    "# #                 nn.Flatten(),\n",
    "# #                 nn.Hardtanh(-10, 10)\n",
    "#             )\n",
    "            \n",
    "            # Classify layers\n",
    "            self.cls_layer_c = nn.Sequential(\n",
    "                nn.Linear(latent_dim, c_dim),\n",
    "            )\n",
    "            \n",
    "            # classifying groups\n",
    "#             self.cls_layer_g = nn.Sequential(\n",
    "#                 nn.Linear(latent_dim, g_dim),\n",
    "#             )\n",
    "\n",
    "        def forward(self, data):\n",
    "          #  data = torch.cat((data, label), -1)\n",
    "#             atten = self.self_att(data)\n",
    "            out = self.model(data)\n",
    "#             out = self.residual(out)\n",
    "            # shortcut connection with attention embeddings\n",
    "#             out = combine(out, atten)\n",
    "            validity = [adv(out) for adv in self.adv_layer]\n",
    "#             validity = self.adv_layer(out)\n",
    "    \n",
    "            classify_c = self.cls_layer_c(out)\n",
    "#             classify_g = self.cls_layer_g(out)\n",
    "            return validity, classify_c\n",
    "\n",
    "    def discriminator() -> Discriminator:\n",
    "        # weird trick\n",
    "        model = Discriminator()\n",
    "        return model\n",
    "\n",
    "    # Initialize generator and discriminator\n",
    "    G_AB = Generator()\n",
    "    D_B = discriminator()\n",
    "#     D_latent = Discriminator_latent()\n",
    "    \n",
    "    target = [int(x) for x in scd.org_batch]\n",
    "    \n",
    "    class_sample_count = np.array(\n",
    "        [len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in target])\n",
    "\n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    samples_weight = samples_weight.double()\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight) * 2)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset = scd,\n",
    "        batch_size=2048,\n",
    "        shuffle=False, sampler = sampler, drop_last = True\n",
    "    )\n",
    "    \n",
    "#     def criterion_cycle(x_reconst, x_real):\n",
    "#         return torch.mean(torch.abs(x_real - x_reconst))\n",
    "    \n",
    "    criterion_cycle = nn.SmoothL1Loss(reduction = \"mean\")\n",
    "#     criterion_mmd = MMDLoss()\n",
    "#     criterion_batch = KLLossBatches()\n",
    "#     criterion_match = MMDLoss()\n",
    "#     criterion_similar = EfficientSupConLoss()\n",
    "#     criterion_batch_fake = JSLossBatches()\n",
    "#     criterion_cycle_dist = CombineLoss()\n",
    "    \n",
    "    def criterion_cls(logit, target):\n",
    "        return F.binary_cross_entropy_with_logits(logit, target, size_average=False) / logit.size(0)\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    criterion_c = criterion_cls\n",
    "    criterion_g = criterion_cls\n",
    "#     loss_fn = nn.BCEWithLogitsLoss()\n",
    "#     criterion_mixing = nn.CrossEntropyLoss(label_smoothing=0.0)\n",
    "#     criterion_c_latent = nn.CrossEntropyLoss(label_smoothing=0.0)\n",
    "#     crtierion_div = DistributionMinimization()\n",
    "#     criterion_g = nn.CrossEntropyLoss()\n",
    "    \n",
    "#     criterion_cls = SGVLB(D_B, len(dataloader.dataset), F.binary_cross_entropy_with_logits)\n",
    "    \n",
    "    def l1_norm(x):\n",
    "        return torch.mean(torch.abs(x))\n",
    "\n",
    "    # Loss weights\n",
    "    lambda_cls = 1\n",
    "    lambda_rec = 10\n",
    "    lambda_gp = 1\n",
    "    \n",
    "#     lambda_style = 5\n",
    "    lambda_adv = 1\n",
    "    \n",
    "    if cuda:\n",
    "        G_AB.cuda()\n",
    "        D_B.cuda()\n",
    "#         D_latent.cuda()\n",
    "        criterion_cycle.cuda()\n",
    "#         criterion_mmd.cuda()\n",
    "#         loss_fn.cuda()\n",
    "#         criterion_similar.cuda()\n",
    "#         criterion_c.cuda()\n",
    "#         criterion_mixing.cuda()\n",
    "#         criterion_match.cuda()\n",
    "#         criterion_batch.cuda()\n",
    "#         criterion_c_latent.cuda()\n",
    "    \n",
    "    # Initialize weights\n",
    "    G_AB.apply(weights_init_normal)\n",
    "    D_B.apply(weights_init_normal)\n",
    "    \n",
    "    # adding optimiser\n",
    "#     optimizer_G_AB = torch.optim.Adam(G_AB.parameters(), lr=lr, betas=(b1, b2)) # can add W to adam\n",
    "#     optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "    optimizer_G_AB = DoG(G_AB.parameters(), eps = 1e-16, reps_rel = 1e-4) # can add W to adam\n",
    "    optimizer_D_B = DoG(D_B.parameters(), eps = 1e-16, reps_rel = 1e-4)\n",
    "    \n",
    "    # adding averager based on author recommendation\n",
    "    global averager_G\n",
    "    global averager_D\n",
    "    averager_G = PolynomialDecayAverager(G_AB, 4)\n",
    "    averager_D = PolynomialDecayAverager(D_B, 4)\n",
    "    \n",
    "    # weight decay parameter is the same as AdamW\n",
    "#     optimizer_G_AB = ACProp(G_AB.parameters(), lr=lr, betas=(b1, b2), eps=1e-16, weight_decouple = True, rectify = True, amsgrad = True, weight_decay = 0)  \n",
    "#     optimizer_D_B = ACProp(D_B.parameters(), lr=lr, betas=(b1, b2), eps=1e-16, weight_decouple = True, rectify = True, amsgrad = True, weight_decay = 0)\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    G_AB.train()\n",
    "    D_B.train()\n",
    "\n",
    "    def generate_data(G_AB, imgs, sampled_c):\n",
    "        fake_imgs = G_AB(imgs, sampled_c)\n",
    "        fake_imgs = F.relu(imgs + fake_imgs)\n",
    "        return fake_imgs\n",
    "    \n",
    "    def get_adv_loss(valid):\n",
    "        adv_loss = 0\n",
    "        for i in range(c_dim):\n",
    "            adv_loss += torch.mean(valid[i])\n",
    "        return adv_loss\n",
    "    \n",
    "    def get_adv(valid):\n",
    "        adv_loss = 0\n",
    "        for i in range(c_dim):\n",
    "            adv_loss += valid[i]\n",
    "        return adv_loss\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "#         if (epoch - 1) == explanationSwitch:\n",
    "# #             def print_grad(module, grad_input, grad_output):\n",
    "# #                 print(grad_input[0].shape)\n",
    "#             G_AB.register_backward_hook(explanation_hook)\n",
    "#             local_explainable = True\n",
    "#         kl_weight = min(kl_weight+0.02, 1)\n",
    "\n",
    "        # Anneal the dropout rate for the current epoch\n",
    "#         dropout_rate = anneal_dropout_rate(epoch)\n",
    "#         G_AB.apply(lambda module: setattr(module, \"dropout_rate\", dropout_rate))\n",
    "#         D_B.apply(lambda module: setattr(module, \"dropout_rate\", dropout_rate))\n",
    "        \n",
    "        for i, (X, labels, one_hot_labels, groups, one_hot_groups) in enumerate(dataloader):\n",
    "            batch_size = X.shape[0]\n",
    "\n",
    "            # Model inputs\n",
    "            imgs = Variable(X.type(FloatTensor), requires_grad=True)       \n",
    "            labels = Variable(labels.type(LongTensor))\n",
    "            one_hot_labels = Variable(one_hot_labels.type(FloatTensor))\n",
    "            groups = Variable(groups.type(LongTensor))\n",
    "            one_hot_groups = Variable(one_hot_groups.type(FloatTensor))\n",
    "#             fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "            \n",
    "            # sample noise\n",
    "#             z = Variable(Tensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
    "\n",
    "            # Sample labels as generator inputs\n",
    "#             sampled_c = Variable(FloatTensor(np.random.randint(0, 2, (imgs.size(0), c_dim))))\n",
    "            sampled_c = Variable(LongTensor(np.random.choice(c_dim, batch_size).astype('long')))\n",
    "            sampled_g = Variable(LongTensor(np.random.choice(g_dim, batch_size).astype('long')))\n",
    "#             sampled_c_2 = Variable(LongTensor(np.random.choice(c_dim, batch_size).astype('long')))\n",
    "#             sampled_g = Variable(LongTensor(np.random.choice(g_dim, batch_size).astype('long')))\n",
    "#             sampled_c = Variable(sampled_c.type(FloatTensor))\n",
    "\n",
    "            # Generate fake batch of images\n",
    "            # adding noise to the generated images\n",
    "            fake_imgs = Variable(generate_data(G_AB, imgs, sampled_c), requires_grad = True)\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            optimizer_D_B.zero_grad()\n",
    "#             optimizer_D_latent.zero_grad()\n",
    "#             optimizer_G_AB.zero_grad()\n",
    "            \n",
    "            # real images\n",
    "            real_validity, pred_c = D_B(imgs)\n",
    "            # Fake images\n",
    "            fake_validity, _, = D_B(fake_imgs)\n",
    "            # gradient penalty\n",
    "#             gradient_penalty = calculate_gradient_penalty(imgs.data, fake_imgs.data, D_B)\n",
    "            gradient_penalty = calculate_div_gp(get_adv(real_validity), get_adv(fake_validity), imgs, fake_imgs)\n",
    "    \n",
    "            # Adversarial loss\n",
    "#             real_validity = penalty_normalize_gradient(D_B, imgs)\n",
    "#             fake_validity = penalty_normalize_gradient(D_B, fake_imgs)\n",
    "            \n",
    "            loss_D_adv = get_adv_loss(real_validity) - get_adv_loss(fake_validity) + gradient_penalty\n",
    "#             loss_D_mmd = criterion_mmd(imgs, fake_imgs)\n",
    "#             print(loss_D_mmd)\n",
    "             \n",
    "#             combined_input = combine(imgs, G_AB.embedding_layer_c(labels))\n",
    "            \n",
    "#             real_imgs_latent = G_AB.encoder(combined_input) + G_AB.atten_out(combined_input) + G_AB.random_atten_out(combined_input)\n",
    "            \n",
    "            # Classification loss\n",
    "#             loss_D_cls = criterion_c(pred_c, labels) + criterion_c_latent(D_latent(real_imgs_latent), labels)\n",
    "            loss_D_cls = criterion_c(pred_c, one_hot_labels)\n",
    "    \n",
    "#             print(loss_D_cls)\n",
    "            \n",
    "            # total Loss\n",
    "            loss_D = lambda_adv * loss_D_adv + lambda_cls * (loss_D_cls)\n",
    "#             loss_D = lambda_adv * loss_D_adv + lambda_cls * loss_D_cls\n",
    "            \n",
    "#             scaler_D.scale(loss_D).backward()\n",
    "#             scaler_D.step(optimizer_D_B)\n",
    "#             scaler_D.update()\n",
    "            \n",
    "            loss_D.backward()\n",
    "#             clip_grad_norm(loss_D)\n",
    "            optimizer_D_B.step()\n",
    "            averager_D.step()\n",
    "            \n",
    "#             for p in D_B.parameters():\n",
    "#                 p.data.clamp_(-2.0, 2.0)\n",
    "#             optimizer_D_latent.step()\n",
    "            \n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "            \n",
    "            \n",
    "            if i % n_critic == 0:\n",
    "                \n",
    "                optimizer_G_AB.zero_grad()\n",
    "                # Translate and reconstruct image\n",
    "                # adding noise again\n",
    "                gen_imgs = generate_data(G_AB, imgs, sampled_c)\n",
    "                recov_imgs = generate_data(G_AB, gen_imgs, labels)\n",
    "                \n",
    "                # Discriminator evaluates translated image\n",
    "                fake_validity, pred_c_2 = D_B(gen_imgs)\n",
    "            \n",
    "                loss_G_adv = get_adv_loss(fake_validity)\n",
    "                \n",
    "                # experimental style vector loss function\n",
    "#                 imgs_style = gen_imgs - imgs\n",
    "#                 recov_style = gen_imgs - recov_imgs\n",
    "#                 loss_style = F.smooth_l1_loss(imgs_style, recov_style)\n",
    "            \n",
    "                # classification loss\n",
    "                loss_G_cls = criterion_c(pred_c_2, F.one_hot(sampled_c).float())\n",
    "                \n",
    "                # Cycle consistency Reconstruction loss\n",
    "                loss_G_rec = criterion_cycle(recov_imgs, imgs)\n",
    "                \n",
    "                # total loss\n",
    "                loss_G = lambda_adv * loss_G_adv + lambda_rec * loss_G_rec + lambda_cls * (loss_G_cls) \n",
    "                loss_G.backward()\n",
    "#                 clip_grad_norm(loss_G)\n",
    "                optimizer_G_AB.step()\n",
    "                averager_G.step()\n",
    "\n",
    "\n",
    "                # --------------\n",
    "                #  Log Progress\n",
    "                # --------------\n",
    "\n",
    "            # Determine approximate time left\n",
    "            batches_done = epoch * len(dataloader) + i\n",
    "            batches_left = n_epochs * len(dataloader) - batches_done\n",
    "            time_left = datetime.timedelta(\n",
    "                seconds=batches_left * (time.time() - start_time) / (batches_done + 1))\n",
    "\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D adv: %f, aux: %f] [G loss: %f, adv: %f, cls: %f, cycle: %f] ETA: %s\"\n",
    "            % (\n",
    "                epoch,\n",
    "                n_epochs,\n",
    "                i,\n",
    "                len(dataloader),\n",
    "                loss_D_adv.item(),\n",
    "                loss_D_cls.item(),\n",
    "                loss_G.item(),\n",
    "                loss_G_adv.item(),\n",
    "                loss_G_cls.item(),\n",
    "                loss_G_rec.item(),\n",
    "#                 loss_G_mse.item(),\n",
    "                time_left,\n",
    "            )\n",
    "        )\n",
    "        #################\n",
    "        # implement early stopping for 10 iterations\n",
    "        ###################\n",
    "        \n",
    "        # need to train the model at least 500 epochs first\n",
    "#         if epoch < 1200:\n",
    "#             continue\n",
    "        \n",
    "#         monitor_loss = loss_G_cls.item()\n",
    "        \n",
    "#         if monitor_loss > best_loss:\n",
    "#             trigger_times += 1\n",
    "            \n",
    "#             if trigger_times > 30:\n",
    "#                 return G_AB_best, D_B_best\n",
    "            \n",
    "# #             print(trigger_times)\n",
    "            \n",
    "#         else:\n",
    "#             best_loss = monitor_loss\n",
    "#             trigger_times = 0\n",
    "#             G_AB_best = G_AB\n",
    "#             D_B_best = D_B\n",
    "        \n",
    "#         if monitor_loss > best_loss + 0.5:\n",
    "#             best_loss = 100.0 # reset best loss function when the fluctuation is too strong\n",
    "        \n",
    "#         if best_score < loss_G_adv.item():\n",
    "#             G_AB_best = G_AB\n",
    "#             best_score = loss_G_adv.item()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return G_AB, D_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ad56ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X = adata.layers[\"log_norm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7d2b7922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adata Info: \n",
      "AnnData object with n_obs × n_vars = 7891 × 4695\n",
      "    obs: 'Batch', 'Group', 'ExpLibSize', 'Batch_num', 'Group_num', 'n_genes', 'n_counts', 'leiden'\n",
      "    var: 'n_cells'\n",
      "    uns: 'log1p', 'neighbors', 'umap', 'Batch_colors', 'Group_num_colors', 'leiden'\n",
      "    obsm: 'X_latent', 'X_pca', 'X_umap'\n",
      "    layers: 'counts', 'log_norm', 'X_raw'\n",
      "    obsp: 'distances', 'connectivities'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rom/miniconda3/envs/genex/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/600] [Batch 6/7] [D adv: 0.000266, aux: 2.724010] [G loss: 2.690032, adv: -0.043739, cls: 2.727513, cycle: 0.000626] ETA: 0:17:22.325448\n",
      "[Epoch 1/600] [Batch 6/7] [D adv: -0.000016, aux: 2.474585] [G loss: 2.486753, adv: -0.035019, cls: 2.515970, cycle: 0.000580] ETA: 0:19:44.938010\n",
      "[Epoch 2/600] [Batch 6/7] [D adv: -0.000045, aux: 1.885369] [G loss: 2.322534, adv: 0.020448, cls: 2.296942, cycle: 0.000514] ETA: 0:19:26.265789\n",
      "[Epoch 3/600] [Batch 6/7] [D adv: 0.000378, aux: 0.454399] [G loss: 3.883363, adv: 0.154621, cls: 3.723764, cycle: 0.000498] ETA: 0:19:07.570743\n",
      "[Epoch 4/600] [Batch 6/7] [D adv: 0.000411, aux: 0.061426] [G loss: 6.691633, adv: 0.224883, cls: 6.458845, cycle: 0.000790] ETA: 0:19:07.089335\n",
      "[Epoch 5/600] [Batch 6/7] [D adv: -0.000184, aux: 0.026891] [G loss: 7.989527, adv: 0.214801, cls: 7.756103, cycle: 0.001862] ETA: 0:19:08.258371\n",
      "[Epoch 6/600] [Batch 6/7] [D adv: -0.004698, aux: 0.016078] [G loss: 8.617785, adv: 0.227487, cls: 8.342670, cycle: 0.004763] ETA: 0:19:23.663696\n",
      "[Epoch 7/600] [Batch 6/7] [D adv: -0.045201, aux: 0.158611] [G loss: 7.593524, adv: 2.627028, cls: 4.772112, cycle: 0.019438] ETA: 0:19:33.776911\n",
      "[Epoch 8/600] [Batch 6/7] [D adv: -1.077164, aux: 0.015413] [G loss: 7.622466, adv: 2.270288, cls: 4.702328, cycle: 0.064985] ETA: 0:19:42.720982\n",
      "[Epoch 9/600] [Batch 6/7] [D adv: -1.157987, aux: 0.017643] [G loss: 3.261906, adv: -0.778373, cls: 2.490361, cycle: 0.154992] ETA: 0:19:54.494379\n",
      "[Epoch 10/600] [Batch 6/7] [D adv: -0.576948, aux: 0.026871] [G loss: 2.532928, adv: -0.034431, cls: 0.928139, cycle: 0.163922] ETA: 0:19:56.573498\n",
      "[Epoch 11/600] [Batch 6/7] [D adv: -0.634074, aux: 0.037364] [G loss: -0.614657, adv: -2.451876, cls: 0.389713, cycle: 0.144751] ETA: 0:20:00.726801\n",
      "[Epoch 12/600] [Batch 6/7] [D adv: -0.256911, aux: 0.283983] [G loss: -7.954585, adv: -9.749075, cls: 0.449449, cycle: 0.134504] ETA: 0:20:05.565469\n",
      "[Epoch 13/600] [Batch 6/7] [D adv: -0.421316, aux: 0.088066] [G loss: -8.635597, adv: -9.935283, cls: 0.204580, cycle: 0.109510] ETA: 0:20:09.506662\n",
      "[Epoch 14/600] [Batch 6/7] [D adv: -0.509565, aux: 0.068273] [G loss: -9.021141, adv: -10.237283, cls: 0.208858, cycle: 0.100728] ETA: 0:20:10.656157\n",
      "[Epoch 15/600] [Batch 6/7] [D adv: -0.618440, aux: 0.040320] [G loss: -9.066715, adv: -10.235109, cls: 0.141872, cycle: 0.102652] ETA: 0:20:09.076663\n",
      "[Epoch 16/600] [Batch 6/7] [D adv: -0.521507, aux: 0.031611] [G loss: -9.267030, adv: -10.430581, cls: 0.113954, cycle: 0.104960] ETA: 0:20:38.414410\n",
      "[Epoch 17/600] [Batch 6/7] [D adv: -0.634688, aux: 0.028320] [G loss: -9.345179, adv: -10.501055, cls: 0.097432, cycle: 0.105844] ETA: 0:20:46.350820\n",
      "[Epoch 18/600] [Batch 6/7] [D adv: -0.679172, aux: 0.022799] [G loss: -9.809514, adv: -11.004131, cls: 0.095460, cycle: 0.109916] ETA: 0:20:37.362178\n",
      "[Epoch 19/600] [Batch 6/7] [D adv: -0.870364, aux: 0.044931] [G loss: -9.179560, adv: -10.506010, cls: 0.195808, cycle: 0.113064] ETA: 0:20:27.188240\n",
      "[Epoch 20/600] [Batch 6/7] [D adv: -0.900720, aux: 0.021833] [G loss: -9.205140, adv: -10.477962, cls: 0.139333, cycle: 0.113349] ETA: 0:20:19.551384\n",
      "[Epoch 21/600] [Batch 6/7] [D adv: -0.717224, aux: 0.037767] [G loss: -7.571313, adv: -8.919997, cls: 0.226775, cycle: 0.112191] ETA: 0:20:15.163550\n",
      "[Epoch 22/600] [Batch 6/7] [D adv: -0.861659, aux: 0.013806] [G loss: -9.419386, adv: -10.693460, cls: 0.059484, cycle: 0.121459] ETA: 0:20:14.751965\n",
      "[Epoch 23/600] [Batch 6/7] [D adv: -1.532634, aux: 0.019515] [G loss: -7.692070, adv: -9.085609, cls: 0.207428, cycle: 0.118611] ETA: 0:20:11.088005\n",
      "[Epoch 24/600] [Batch 6/7] [D adv: 0.134595, aux: 0.020245] [G loss: -7.129813, adv: -8.494520, cls: 0.266939, cycle: 0.109777] ETA: 0:20:07.586793\n",
      "[Epoch 25/600] [Batch 6/7] [D adv: -0.432146, aux: 0.033803] [G loss: -12.817881, adv: -13.926249, cls: 0.101704, cycle: 0.100666] ETA: 0:20:05.014324\n",
      "[Epoch 26/600] [Batch 6/7] [D adv: -0.615650, aux: 0.024582] [G loss: -13.760676, adv: -14.872363, cls: 0.086145, cycle: 0.102554] ETA: 0:20:00.282939\n",
      "[Epoch 27/600] [Batch 6/7] [D adv: -0.654123, aux: 0.020494] [G loss: -14.631993, adv: -15.749901, cls: 0.074259, cycle: 0.104365] ETA: 0:19:57.164467\n",
      "[Epoch 28/600] [Batch 6/7] [D adv: -0.704392, aux: 0.015892] [G loss: -15.577038, adv: -16.740345, cls: 0.063993, cycle: 0.109931] ETA: 0:19:53.805676\n",
      "[Epoch 29/600] [Batch 6/7] [D adv: -0.849211, aux: 0.011789] [G loss: -15.909684, adv: -17.124035, cls: 0.091975, cycle: 0.112238] ETA: 0:19:51.012064\n",
      "[Epoch 30/600] [Batch 6/7] [D adv: -0.320071, aux: 0.015016] [G loss: -16.617895, adv: -17.822330, cls: 0.053213, cycle: 0.115122] ETA: 0:19:48.780991\n",
      "[Epoch 31/600] [Batch 6/7] [D adv: -0.792952, aux: 0.011812] [G loss: -17.618780, adv: -18.758259, cls: 0.055177, cycle: 0.108430] ETA: 0:19:45.984791\n",
      "[Epoch 32/600] [Batch 6/7] [D adv: -0.163668, aux: 0.015555] [G loss: -18.982725, adv: -20.239666, cls: 0.114873, cycle: 0.114207] ETA: 0:19:44.721307\n",
      "[Epoch 33/600] [Batch 6/7] [D adv: -1.321007, aux: 0.008725] [G loss: -18.532238, adv: -19.668140, cls: 0.039855, cycle: 0.109605] ETA: 0:19:41.653532\n",
      "[Epoch 34/600] [Batch 6/7] [D adv: -1.789046, aux: 0.007057] [G loss: -17.890434, adv: -19.072945, cls: 0.091088, cycle: 0.109142] ETA: 0:19:38.312361\n",
      "[Epoch 35/600] [Batch 6/7] [D adv: -1.360344, aux: 0.008175] [G loss: -19.163771, adv: -20.279053, cls: 0.104747, cycle: 0.101053] ETA: 0:19:34.696096\n",
      "[Epoch 36/600] [Batch 6/7] [D adv: -1.663748, aux: 0.006891] [G loss: -19.497620, adv: -20.548801, cls: 0.078225, cycle: 0.097296] ETA: 0:19:32.103702\n",
      "[Epoch 37/600] [Batch 6/7] [D adv: -1.872914, aux: 0.004969] [G loss: -17.908508, adv: -19.020514, cls: 0.107132, cycle: 0.100487] ETA: 0:19:29.923950\n",
      "[Epoch 38/600] [Batch 6/7] [D adv: -1.535998, aux: 0.005481] [G loss: -19.446373, adv: -20.501699, cls: 0.043727, cycle: 0.101160] ETA: 0:19:28.355402\n",
      "[Epoch 39/600] [Batch 6/7] [D adv: -2.383332, aux: 0.005072] [G loss: -20.255613, adv: -21.340918, cls: 0.035395, cycle: 0.104991] ETA: 0:19:26.247002\n",
      "[Epoch 40/600] [Batch 6/7] [D adv: -0.171984, aux: 0.006100] [G loss: -21.126688, adv: -22.160183, cls: 0.037480, cycle: 0.099602] ETA: 0:19:24.884293\n",
      "[Epoch 41/600] [Batch 6/7] [D adv: -1.672396, aux: 0.005953] [G loss: -19.459953, adv: -20.576824, cls: 0.101875, cycle: 0.101500] ETA: 0:19:22.439571\n",
      "[Epoch 42/600] [Batch 6/7] [D adv: -1.687832, aux: 0.003199] [G loss: -19.073595, adv: -20.133410, cls: 0.031031, cycle: 0.102878] ETA: 0:19:19.649111\n",
      "[Epoch 43/600] [Batch 6/7] [D adv: -1.360700, aux: 0.004862] [G loss: -23.816513, adv: -24.860771, cls: 0.032724, cycle: 0.101153] ETA: 0:19:17.092423\n",
      "[Epoch 44/600] [Batch 6/7] [D adv: -1.769091, aux: 0.002708] [G loss: -22.840042, adv: -23.850254, cls: 0.024233, cycle: 0.098598] ETA: 0:19:14.364171\n",
      "[Epoch 45/600] [Batch 6/7] [D adv: -2.742821, aux: 0.002697] [G loss: -20.748857, adv: -21.724731, cls: 0.038499, cycle: 0.093737] ETA: 0:19:12.214474\n",
      "[Epoch 46/600] [Batch 6/7] [D adv: -2.026982, aux: 0.002918] [G loss: -19.807772, adv: -20.817200, cls: 0.043958, cycle: 0.096547] ETA: 0:19:09.741376\n",
      "[Epoch 47/600] [Batch 6/7] [D adv: -0.699224, aux: 0.003489] [G loss: -21.203161, adv: -22.161770, cls: 0.018087, cycle: 0.094052] ETA: 0:19:07.783375\n",
      "[Epoch 48/600] [Batch 6/7] [D adv: -1.373860, aux: 0.002958] [G loss: -19.843525, adv: -20.906630, cls: 0.028054, cycle: 0.103505] ETA: 0:19:05.368089\n",
      "[Epoch 49/600] [Batch 6/7] [D adv: -1.852345, aux: 0.002268] [G loss: -19.687408, adv: -20.780289, cls: 0.020220, cycle: 0.107266] ETA: 0:19:04.179147\n",
      "[Epoch 50/600] [Batch 6/7] [D adv: -0.420350, aux: 0.004844] [G loss: -13.648149, adv: -14.727430, cls: 0.024959, cycle: 0.105432] ETA: 0:19:01.573798\n",
      "[Epoch 51/600] [Batch 6/7] [D adv: -0.614498, aux: 0.003596] [G loss: -13.135941, adv: -14.159932, cls: 0.009401, cycle: 0.101459] ETA: 0:18:57.349814\n",
      "[Epoch 52/600] [Batch 6/7] [D adv: -0.680780, aux: 0.003260] [G loss: -12.898993, adv: -13.962716, cls: 0.012113, cycle: 0.105161] ETA: 0:18:54.605422\n",
      "[Epoch 53/600] [Batch 6/7] [D adv: -0.813823, aux: 0.003349] [G loss: -13.153042, adv: -14.254843, cls: 0.022923, cycle: 0.107888] ETA: 0:18:50.878775\n",
      "[Epoch 54/600] [Batch 6/7] [D adv: -0.887993, aux: 0.003165] [G loss: -14.463878, adv: -15.586779, cls: 0.016163, cycle: 0.110674] ETA: 0:18:47.782936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 55/600] [Batch 6/7] [D adv: -1.425608, aux: 0.004437] [G loss: -14.979241, adv: -16.119034, cls: 0.025308, cycle: 0.111448] ETA: 0:18:45.762051\n",
      "[Epoch 56/600] [Batch 6/7] [D adv: -1.207008, aux: 0.003263] [G loss: -16.613445, adv: -17.767818, cls: 0.020160, cycle: 0.113421] ETA: 0:18:43.647072\n",
      "[Epoch 57/600] [Batch 6/7] [D adv: -2.030592, aux: 0.004527] [G loss: -14.864842, adv: -16.019627, cls: 0.029264, cycle: 0.112552] ETA: 0:18:41.780411\n",
      "[Epoch 58/600] [Batch 6/7] [D adv: -0.673148, aux: 0.003004] [G loss: -15.520684, adv: -16.576864, cls: 0.028627, cycle: 0.102755] ETA: 0:18:39.556615\n",
      "[Epoch 59/600] [Batch 6/7] [D adv: -1.610341, aux: 0.003771] [G loss: -12.724441, adv: -13.883021, cls: 0.079202, cycle: 0.107938] ETA: 0:18:36.847806\n",
      "[Epoch 60/600] [Batch 6/7] [D adv: -2.340872, aux: 0.006186] [G loss: -11.268538, adv: -12.377352, cls: 0.051186, cycle: 0.105763] ETA: 0:18:34.675761\n",
      "[Epoch 61/600] [Batch 6/7] [D adv: -2.131504, aux: 0.002149] [G loss: -12.137202, adv: -13.118489, cls: 0.032374, cycle: 0.094891] ETA: 0:18:31.400691\n",
      "[Epoch 62/600] [Batch 6/7] [D adv: -1.530169, aux: 0.002807] [G loss: -13.968093, adv: -15.030317, cls: 0.032054, cycle: 0.103017] ETA: 0:18:29.105399\n",
      "[Epoch 63/600] [Batch 6/7] [D adv: -1.942877, aux: 0.003834] [G loss: -10.911313, adv: -11.951012, cls: 0.049478, cycle: 0.099022] ETA: 0:18:26.151001\n",
      "[Epoch 64/600] [Batch 6/7] [D adv: -1.101061, aux: 0.002658] [G loss: -16.603638, adv: -17.612267, cls: 0.013801, cycle: 0.099483] ETA: 0:18:24.143642\n",
      "[Epoch 65/600] [Batch 6/7] [D adv: -1.848341, aux: 0.002162] [G loss: -15.059138, adv: -16.124516, cls: 0.016735, cycle: 0.104864] ETA: 0:18:20.367221\n",
      "[Epoch 66/600] [Batch 6/7] [D adv: -3.012475, aux: 0.002080] [G loss: -12.246449, adv: -13.332999, cls: 0.067379, cycle: 0.101917] ETA: 0:18:16.468850\n",
      "[Epoch 67/600] [Batch 6/7] [D adv: -2.457161, aux: 0.002548] [G loss: -14.818221, adv: -15.769951, cls: 0.050521, cycle: 0.090121] ETA: 0:18:13.614013\n",
      "[Epoch 68/600] [Batch 6/7] [D adv: -3.973355, aux: 0.001894] [G loss: -16.317425, adv: -17.207037, cls: 0.043935, cycle: 0.084568] ETA: 0:18:11.727589\n",
      "[Epoch 69/600] [Batch 6/7] [D adv: -2.205306, aux: 0.001787] [G loss: -17.655508, adv: -18.636972, cls: 0.019409, cycle: 0.096205] ETA: 0:18:09.753447\n",
      "[Epoch 70/600] [Batch 6/7] [D adv: -2.209046, aux: 0.001563] [G loss: -20.365698, adv: -21.309118, cls: 0.017937, cycle: 0.092548] ETA: 0:18:07.414787\n",
      "[Epoch 71/600] [Batch 6/7] [D adv: -2.672474, aux: 0.004141] [G loss: -14.768675, adv: -15.722089, cls: 0.096433, cycle: 0.085698] ETA: 0:18:05.267361\n",
      "[Epoch 72/600] [Batch 6/7] [D adv: -2.494745, aux: 0.002104] [G loss: -22.380903, adv: -23.193720, cls: 0.091908, cycle: 0.072091] ETA: 0:18:02.929457\n",
      "[Epoch 73/600] [Batch 6/7] [D adv: -3.630829, aux: 0.002101] [G loss: -18.692234, adv: -19.609798, cls: 0.137267, cycle: 0.078030] ETA: 0:18:00.470809\n",
      "[Epoch 74/600] [Batch 6/7] [D adv: -1.267277, aux: 0.002850] [G loss: -19.017574, adv: -20.044758, cls: 0.160117, cycle: 0.086707] ETA: 0:17:58.201280\n",
      "[Epoch 75/600] [Batch 6/7] [D adv: -0.906487, aux: 0.002718] [G loss: -21.908897, adv: -22.736208, cls: 0.025448, cycle: 0.080186] ETA: 0:17:55.908429\n",
      "[Epoch 76/600] [Batch 6/7] [D adv: -1.266641, aux: 0.002596] [G loss: -22.286190, adv: -23.135351, cls: 0.024545, cycle: 0.082462] ETA: 0:17:53.970432\n",
      "[Epoch 77/600] [Batch 6/7] [D adv: -1.309005, aux: 0.002471] [G loss: -20.886253, adv: -21.759848, cls: 0.012993, cycle: 0.086060] ETA: 0:17:51.340246\n",
      "[Epoch 78/600] [Batch 6/7] [D adv: -0.285757, aux: 0.002534] [G loss: -18.427563, adv: -19.327374, cls: 0.038870, cycle: 0.086094] ETA: 0:17:48.530156\n",
      "[Epoch 79/600] [Batch 6/7] [D adv: -0.738968, aux: 0.003321] [G loss: -20.161882, adv: -21.018888, cls: 0.015145, cycle: 0.084186] ETA: 0:17:44.567901\n",
      "[Epoch 80/600] [Batch 6/7] [D adv: -1.504277, aux: 0.002160] [G loss: -17.136082, adv: -17.991386, cls: 0.009103, cycle: 0.084620] ETA: 0:17:41.121878\n",
      "[Epoch 81/600] [Batch 6/7] [D adv: -1.843091, aux: 0.001747] [G loss: -17.008553, adv: -17.783772, cls: 0.010009, cycle: 0.076521] ETA: 0:17:37.576334\n",
      "[Epoch 82/600] [Batch 6/7] [D adv: -2.796802, aux: 0.002141] [G loss: -15.348829, adv: -16.100721, cls: 0.055382, cycle: 0.069651] ETA: 0:17:35.067804\n",
      "[Epoch 83/600] [Batch 6/7] [D adv: -2.703718, aux: 0.006772] [G loss: -11.968839, adv: -13.050307, cls: 0.245255, cycle: 0.083621] ETA: 0:17:33.047876\n",
      "[Epoch 84/600] [Batch 6/7] [D adv: -4.588611, aux: 0.001664] [G loss: -14.134866, adv: -14.935235, cls: 0.030023, cycle: 0.077035] ETA: 0:17:31.041518\n",
      "[Epoch 85/600] [Batch 6/7] [D adv: -3.228711, aux: 0.001916] [G loss: -19.314724, adv: -19.993893, cls: 0.030415, cycle: 0.064875] ETA: 0:17:28.929905\n",
      "[Epoch 86/600] [Batch 6/7] [D adv: -3.020574, aux: 0.002575] [G loss: -19.058464, adv: -19.714432, cls: 0.060913, cycle: 0.059505] ETA: 0:17:26.715672\n",
      "[Epoch 87/600] [Batch 6/7] [D adv: -1.888583, aux: 0.002099] [G loss: -15.737784, adv: -16.496973, cls: 0.204209, cycle: 0.055498] ETA: 0:17:24.811028\n",
      "[Epoch 88/600] [Batch 6/7] [D adv: -3.003348, aux: 0.002197] [G loss: -13.647898, adv: -14.343850, cls: 0.037958, cycle: 0.065799] ETA: 0:17:22.721904\n",
      "[Epoch 89/600] [Batch 6/7] [D adv: -1.964271, aux: 0.002462] [G loss: -16.120102, adv: -16.699516, cls: 0.042555, cycle: 0.053686] ETA: 0:17:20.432464\n",
      "[Epoch 90/600] [Batch 6/7] [D adv: -1.826488, aux: 0.003320] [G loss: -15.125517, adv: -15.695912, cls: 0.052592, cycle: 0.051780] ETA: 0:17:18.332769\n",
      "[Epoch 91/600] [Batch 6/7] [D adv: -1.707836, aux: 0.002831] [G loss: -14.355020, adv: -14.914145, cls: 0.054027, cycle: 0.050510] ETA: 0:17:16.486368\n",
      "[Epoch 92/600] [Batch 6/7] [D adv: 1.209244, aux: 0.004172] [G loss: -13.762177, adv: -14.320487, cls: 0.037630, cycle: 0.052068] ETA: 0:17:14.296195\n",
      "[Epoch 93/600] [Batch 6/7] [D adv: -0.742519, aux: 0.004041] [G loss: -10.474287, adv: -11.012459, cls: 0.038375, cycle: 0.049980] ETA: 0:17:11.872273\n",
      "[Epoch 94/600] [Batch 6/7] [D adv: -0.809103, aux: 0.005459] [G loss: -11.129813, adv: -11.681248, cls: 0.048911, cycle: 0.050252] ETA: 0:17:09.132942\n",
      "[Epoch 95/600] [Batch 6/7] [D adv: -0.504121, aux: 0.004418] [G loss: -8.847580, adv: -9.381956, cls: 0.077592, cycle: 0.045678] ETA: 0:17:06.164678\n",
      "[Epoch 96/600] [Batch 6/7] [D adv: -0.452193, aux: 0.004963] [G loss: -9.375749, adv: -9.874171, cls: 0.036861, cycle: 0.046156] ETA: 0:17:03.613311\n",
      "[Epoch 97/600] [Batch 6/7] [D adv: -0.422088, aux: 0.003646] [G loss: -9.145371, adv: -9.632381, cls: 0.016484, cycle: 0.047053] ETA: 0:17:00.719752\n",
      "[Epoch 98/600] [Batch 6/7] [D adv: -0.675657, aux: 0.004196] [G loss: -7.006883, adv: -7.553568, cls: 0.042675, cycle: 0.050401] ETA: 0:16:58.389317\n",
      "[Epoch 99/600] [Batch 6/7] [D adv: 1.611760, aux: 0.006471] [G loss: -6.602759, adv: -7.139722, cls: 0.039314, cycle: 0.049765] ETA: 0:16:56.742977\n",
      "[Epoch 100/600] [Batch 6/7] [D adv: -0.196481, aux: 0.004114] [G loss: -9.182609, adv: -9.688066, cls: 0.010755, cycle: 0.049470] ETA: 0:16:54.722773\n",
      "[Epoch 101/600] [Batch 6/7] [D adv: -0.179419, aux: 0.004283] [G loss: -9.281868, adv: -9.717362, cls: 0.007796, cycle: 0.042770] ETA: 0:16:53.105864\n",
      "[Epoch 102/600] [Batch 6/7] [D adv: -0.197783, aux: 0.003384] [G loss: -9.274770, adv: -9.709156, cls: 0.010457, cycle: 0.042393] ETA: 0:16:51.178925\n",
      "[Epoch 103/600] [Batch 6/7] [D adv: -0.307044, aux: 0.003861] [G loss: -9.030986, adv: -9.464109, cls: 0.008793, cycle: 0.042433] ETA: 0:16:49.240270\n",
      "[Epoch 104/600] [Batch 6/7] [D adv: -0.613884, aux: 0.002953] [G loss: -7.967750, adv: -8.415450, cls: 0.014710, cycle: 0.043299] ETA: 0:16:47.330873\n",
      "[Epoch 105/600] [Batch 6/7] [D adv: -0.260567, aux: 0.003506] [G loss: -6.761737, adv: -7.208264, cls: 0.008680, cycle: 0.043785] ETA: 0:16:45.338601\n",
      "[Epoch 106/600] [Batch 6/7] [D adv: 0.011544, aux: 0.003434] [G loss: -6.330930, adv: -6.791746, cls: 0.012633, cycle: 0.044818] ETA: 0:16:43.270064\n",
      "[Epoch 107/600] [Batch 6/7] [D adv: 0.042320, aux: 0.003621] [G loss: -3.290608, adv: -3.818028, cls: 0.043357, cycle: 0.048406] ETA: 0:16:41.237270\n",
      "[Epoch 108/600] [Batch 6/7] [D adv: -0.459981, aux: 0.003870] [G loss: -5.371048, adv: -5.804440, cls: 0.011121, cycle: 0.042227] ETA: 0:16:38.923254\n",
      "[Epoch 109/600] [Batch 6/7] [D adv: 0.040424, aux: 0.004362] [G loss: -3.878893, adv: -4.333981, cls: 0.026620, cycle: 0.042847] ETA: 0:16:37.830974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 110/600] [Batch 6/7] [D adv: 0.089470, aux: 0.004247] [G loss: -3.883115, adv: -4.322448, cls: 0.021113, cycle: 0.041822] ETA: 0:16:35.827800\n",
      "[Epoch 111/600] [Batch 6/7] [D adv: -0.276233, aux: 0.003634] [G loss: -3.834188, adv: -4.287282, cls: 0.016894, cycle: 0.043620] ETA: 0:16:33.042471\n",
      "[Epoch 112/600] [Batch 6/7] [D adv: -0.079244, aux: 0.003740] [G loss: -3.142556, adv: -3.609462, cls: 0.024588, cycle: 0.044232] ETA: 0:16:29.767713\n",
      "[Epoch 113/600] [Batch 6/7] [D adv: -0.221386, aux: 0.003643] [G loss: -3.570263, adv: -3.992870, cls: 0.010743, cycle: 0.041186] ETA: 0:16:26.756548\n",
      "[Epoch 114/600] [Batch 6/7] [D adv: -0.250583, aux: 0.003258] [G loss: -2.942298, adv: -3.370681, cls: 0.020840, cycle: 0.040754] ETA: 0:16:23.729345\n",
      "[Epoch 115/600] [Batch 6/7] [D adv: -0.082826, aux: 0.003247] [G loss: -3.007103, adv: -3.415948, cls: 0.013049, cycle: 0.039580] ETA: 0:16:21.317622\n",
      "[Epoch 116/600] [Batch 6/7] [D adv: -0.918233, aux: 0.004504] [G loss: -3.925800, adv: -4.332855, cls: 0.018844, cycle: 0.038821] ETA: 0:16:18.789220\n",
      "[Epoch 117/600] [Batch 6/7] [D adv: -0.230297, aux: 0.003845] [G loss: -2.521765, adv: -2.947023, cls: 0.016848, cycle: 0.040841] ETA: 0:16:16.276224\n",
      "[Epoch 118/600] [Batch 6/7] [D adv: -0.649425, aux: 0.003446] [G loss: -4.661448, adv: -5.025506, cls: 0.010013, cycle: 0.035404] ETA: 0:16:13.964872\n",
      "[Epoch 119/600] [Batch 6/7] [D adv: -0.724234, aux: 0.003687] [G loss: -3.674725, adv: -4.078265, cls: 0.030395, cycle: 0.037315] ETA: 0:16:11.706318\n",
      "[Epoch 120/600] [Batch 6/7] [D adv: -0.552814, aux: 0.003489] [G loss: -3.643515, adv: -4.014178, cls: 0.015505, cycle: 0.035516] ETA: 0:16:09.330809\n",
      "[Epoch 121/600] [Batch 6/7] [D adv: -0.654749, aux: 0.004068] [G loss: -3.384741, adv: -3.755456, cls: 0.014310, cycle: 0.035640] ETA: 0:16:06.563334\n",
      "[Epoch 122/600] [Batch 6/7] [D adv: -0.595131, aux: 0.002871] [G loss: -3.207897, adv: -3.554090, cls: 0.007838, cycle: 0.033836] ETA: 0:16:04.113438\n",
      "[Epoch 123/600] [Batch 6/7] [D adv: -0.591025, aux: 0.002949] [G loss: -2.659955, adv: -3.007196, cls: 0.012086, cycle: 0.033516] ETA: 0:16:01.796553\n",
      "[Epoch 124/600] [Batch 6/7] [D adv: -0.760244, aux: 0.002845] [G loss: -3.253500, adv: -3.581604, cls: 0.010834, cycle: 0.031727] ETA: 0:15:59.465015\n",
      "[Epoch 125/600] [Batch 6/7] [D adv: -0.568795, aux: 0.003581] [G loss: -3.695813, adv: -4.005658, cls: 0.007920, cycle: 0.030192] ETA: 0:15:57.542738\n",
      "[Epoch 126/600] [Batch 6/7] [D adv: -0.507209, aux: 0.003335] [G loss: -3.453861, adv: -3.748334, cls: 0.008581, cycle: 0.028589] ETA: 0:15:55.304843\n",
      "[Epoch 127/600] [Batch 6/7] [D adv: -0.464309, aux: 0.003240] [G loss: -3.608683, adv: -3.886804, cls: 0.006804, cycle: 0.027132] ETA: 0:15:53.422945\n",
      "[Epoch 128/600] [Batch 6/7] [D adv: -0.535359, aux: 0.003280] [G loss: -3.804202, adv: -4.065595, cls: 0.006641, cycle: 0.025475] ETA: 0:15:51.304219\n",
      "[Epoch 129/600] [Batch 6/7] [D adv: -0.560213, aux: 0.002840] [G loss: -3.971166, adv: -4.232363, cls: 0.008146, cycle: 0.025305] ETA: 0:15:49.486054\n",
      "[Epoch 130/600] [Batch 6/7] [D adv: -0.444363, aux: 0.003118] [G loss: -3.867728, adv: -4.125081, cls: 0.007731, cycle: 0.024962] ETA: 0:15:47.400650\n",
      "[Epoch 131/600] [Batch 6/7] [D adv: -0.381740, aux: 0.003182] [G loss: -4.152054, adv: -4.403451, cls: 0.005144, cycle: 0.024625] ETA: 0:15:44.820964\n",
      "[Epoch 132/600] [Batch 6/7] [D adv: -0.367055, aux: 0.002658] [G loss: -4.298317, adv: -4.549478, cls: 0.006740, cycle: 0.024442] ETA: 0:15:42.007534\n",
      "[Epoch 133/600] [Batch 6/7] [D adv: -0.420201, aux: 0.002447] [G loss: -4.248320, adv: -4.497765, cls: 0.007327, cycle: 0.024212] ETA: 0:15:39.430609\n",
      "[Epoch 134/600] [Batch 6/7] [D adv: -0.443959, aux: 0.002515] [G loss: -4.409606, adv: -4.654882, cls: 0.007622, cycle: 0.023765] ETA: 0:15:36.963048\n",
      "[Epoch 135/600] [Batch 6/7] [D adv: -0.405635, aux: 0.002787] [G loss: -4.446394, adv: -4.694244, cls: 0.007330, cycle: 0.024052] ETA: 0:15:35.110053\n",
      "[Epoch 136/600] [Batch 6/7] [D adv: -0.421375, aux: 0.002612] [G loss: -4.838751, adv: -5.079188, cls: 0.007162, cycle: 0.023328] ETA: 0:15:33.212717\n",
      "[Epoch 137/600] [Batch 6/7] [D adv: -0.341467, aux: 0.003128] [G loss: -4.457174, adv: -4.710124, cls: 0.013473, cycle: 0.023948] ETA: 0:15:31.645074\n",
      "[Epoch 138/600] [Batch 6/7] [D adv: -0.398048, aux: 0.002055] [G loss: -5.308655, adv: -5.544703, cls: 0.003794, cycle: 0.023225] ETA: 0:15:30.205734\n",
      "[Epoch 139/600] [Batch 6/7] [D adv: -0.333213, aux: 0.002313] [G loss: -4.791980, adv: -5.026670, cls: 0.004470, cycle: 0.023022] ETA: 0:15:28.300586\n",
      "[Epoch 140/600] [Batch 6/7] [D adv: -0.319624, aux: 0.002334] [G loss: -4.520309, adv: -4.764027, cls: 0.004647, cycle: 0.023907] ETA: 0:15:26.330351\n",
      "[Epoch 141/600] [Batch 6/7] [D adv: -0.340992, aux: 0.001911] [G loss: -4.642003, adv: -4.893878, cls: 0.005024, cycle: 0.024685] ETA: 0:15:24.390242\n",
      "[Epoch 142/600] [Batch 6/7] [D adv: -0.079143, aux: 0.002817] [G loss: -4.349471, adv: -4.610996, cls: 0.005698, cycle: 0.025583] ETA: 0:15:22.468476\n",
      "[Epoch 143/600] [Batch 6/7] [D adv: -0.408294, aux: 0.002682] [G loss: -4.606067, adv: -4.860561, cls: 0.005257, cycle: 0.024924] ETA: 0:15:20.494537\n",
      "[Epoch 144/600] [Batch 6/7] [D adv: -0.155596, aux: 0.001921] [G loss: -3.808720, adv: -4.081091, cls: 0.007567, cycle: 0.026480] ETA: 0:15:18.535294\n",
      "[Epoch 145/600] [Batch 6/7] [D adv: -0.078269, aux: 0.001841] [G loss: -3.529432, adv: -3.815020, cls: 0.010008, cycle: 0.027558] ETA: 0:15:16.581673\n",
      "[Epoch 146/600] [Batch 6/7] [D adv: -0.154019, aux: 0.001913] [G loss: -3.694360, adv: -3.957898, cls: 0.006672, cycle: 0.025687] ETA: 0:15:14.843333\n",
      "[Epoch 147/600] [Batch 6/7] [D adv: -0.500819, aux: 0.001879] [G loss: -3.614054, adv: -3.883413, cls: 0.006747, cycle: 0.026261] ETA: 0:15:12.230237\n",
      "[Epoch 148/600] [Batch 6/7] [D adv: -0.233569, aux: 0.001686] [G loss: -3.206457, adv: -3.498269, cls: 0.006324, cycle: 0.028549] ETA: 0:15:09.397293\n",
      "[Epoch 149/600] [Batch 6/7] [D adv: -0.451104, aux: 0.001541] [G loss: -3.245954, adv: -3.526757, cls: 0.005730, cycle: 0.027507] ETA: 0:15:06.693850\n",
      "[Epoch 150/600] [Batch 6/7] [D adv: -0.410226, aux: 0.001458] [G loss: -2.494146, adv: -2.785774, cls: 0.004384, cycle: 0.028724] ETA: 0:15:03.894868\n",
      "[Epoch 151/600] [Batch 6/7] [D adv: -0.142011, aux: 0.001645] [G loss: -2.282679, adv: -2.568552, cls: 0.005700, cycle: 0.028017] ETA: 0:15:01.118725\n",
      "[Epoch 152/600] [Batch 6/7] [D adv: -0.107678, aux: 0.001720] [G loss: -1.368886, adv: -1.678758, cls: 0.007189, cycle: 0.030268] ETA: 0:14:58.478742\n",
      "[Epoch 153/600] [Batch 6/7] [D adv: -0.046242, aux: 0.001670] [G loss: -1.263428, adv: -1.573492, cls: 0.011229, cycle: 0.029883] ETA: 0:14:55.760383\n",
      "[Epoch 154/600] [Batch 6/7] [D adv: -0.413479, aux: 0.001579] [G loss: -1.857538, adv: -2.140989, cls: 0.005898, cycle: 0.027755] ETA: 0:14:53.039635\n",
      "[Epoch 155/600] [Batch 6/7] [D adv: -0.682985, aux: 0.001474] [G loss: -1.274946, adv: -1.602320, cls: 0.011466, cycle: 0.031591] ETA: 0:14:50.383852\n",
      "[Epoch 156/600] [Batch 6/7] [D adv: -0.074399, aux: 0.001432] [G loss: -1.947316, adv: -2.246990, cls: 0.005058, cycle: 0.029462] ETA: 0:14:47.687529\n",
      "[Epoch 157/600] [Batch 6/7] [D adv: -0.193827, aux: 0.001802] [G loss: -1.753409, adv: -2.076479, cls: 0.010057, cycle: 0.031301] ETA: 0:14:44.994922\n",
      "[Epoch 158/600] [Batch 6/7] [D adv: 0.118931, aux: 0.001639] [G loss: -2.308914, adv: -2.631817, cls: 0.008248, cycle: 0.031465] ETA: 0:14:42.380735\n",
      "[Epoch 159/600] [Batch 6/7] [D adv: -0.084840, aux: 0.001795] [G loss: -2.389008, adv: -2.700238, cls: 0.009861, cycle: 0.030137] ETA: 0:14:39.600730\n",
      "[Epoch 160/600] [Batch 6/7] [D adv: -0.212355, aux: 0.001880] [G loss: -2.654299, adv: -2.972468, cls: 0.013725, cycle: 0.030444] ETA: 0:14:36.943408\n",
      "[Epoch 161/600] [Batch 6/7] [D adv: 0.144001, aux: 0.002262] [G loss: -2.321881, adv: -2.654425, cls: 0.016730, cycle: 0.031581] ETA: 0:14:34.172729\n",
      "[Epoch 162/600] [Batch 6/7] [D adv: 0.123565, aux: 0.002110] [G loss: -3.717095, adv: -3.996311, cls: 0.006713, cycle: 0.027250] ETA: 0:14:31.933186\n",
      "[Epoch 163/600] [Batch 6/7] [D adv: -0.344330, aux: 0.002153] [G loss: -3.222480, adv: -3.530000, cls: 0.009492, cycle: 0.029803] ETA: 0:14:29.664922\n",
      "[Epoch 164/600] [Batch 6/7] [D adv: -0.127910, aux: 0.002207] [G loss: -2.400506, adv: -2.720624, cls: 0.011024, cycle: 0.030909] ETA: 0:14:26.914189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 165/600] [Batch 6/7] [D adv: -0.191677, aux: 0.002025] [G loss: -3.182645, adv: -3.471427, cls: 0.008768, cycle: 0.028001] ETA: 0:14:24.156569\n",
      "[Epoch 166/600] [Batch 6/7] [D adv: -0.492418, aux: 0.001766] [G loss: -2.872521, adv: -3.191826, cls: 0.010980, cycle: 0.030832] ETA: 0:14:21.461422\n",
      "[Epoch 167/600] [Batch 6/7] [D adv: -0.365900, aux: 0.001738] [G loss: -3.047455, adv: -3.342631, cls: 0.005413, cycle: 0.028976] ETA: 0:14:18.669029\n",
      "[Epoch 168/600] [Batch 6/7] [D adv: -0.522189, aux: 0.001652] [G loss: -2.954812, adv: -3.255043, cls: 0.007750, cycle: 0.029248] ETA: 0:14:15.913699\n",
      "[Epoch 169/600] [Batch 6/7] [D adv: -0.528729, aux: 0.001826] [G loss: -2.734504, adv: -3.028453, cls: 0.010219, cycle: 0.028373] ETA: 0:14:13.402589\n",
      "[Epoch 170/600] [Batch 6/7] [D adv: -0.541124, aux: 0.001924] [G loss: -2.896240, adv: -3.185893, cls: 0.008151, cycle: 0.028150] ETA: 0:14:10.616511\n",
      "[Epoch 171/600] [Batch 6/7] [D adv: -0.685217, aux: 0.001970] [G loss: -2.853079, adv: -3.139556, cls: 0.008812, cycle: 0.027767] ETA: 0:14:07.988705\n",
      "[Epoch 172/600] [Batch 6/7] [D adv: -0.390604, aux: 0.002090] [G loss: -2.887169, adv: -3.173699, cls: 0.008782, cycle: 0.027775] ETA: 0:14:05.344043\n",
      "[Epoch 173/600] [Batch 6/7] [D adv: -0.603051, aux: 0.002043] [G loss: -2.798660, adv: -3.085367, cls: 0.008071, cycle: 0.027864] ETA: 0:14:02.672370\n",
      "[Epoch 174/600] [Batch 6/7] [D adv: -0.519197, aux: 0.002083] [G loss: -3.486297, adv: -3.764481, cls: 0.007799, cycle: 0.027038] ETA: 0:14:00.069235\n",
      "[Epoch 175/600] [Batch 6/7] [D adv: -0.549958, aux: 0.002187] [G loss: -3.699043, adv: -3.984167, cls: 0.007398, cycle: 0.027773] ETA: 0:13:57.436910\n",
      "[Epoch 176/600] [Batch 6/7] [D adv: -0.499308, aux: 0.002016] [G loss: -3.950374, adv: -4.223259, cls: 0.009558, cycle: 0.026333] ETA: 0:13:54.769989\n",
      "[Epoch 177/600] [Batch 6/7] [D adv: -0.495330, aux: 0.001857] [G loss: -4.650408, adv: -4.918085, cls: 0.004841, cycle: 0.026284] ETA: 0:13:52.200488\n",
      "[Epoch 178/600] [Batch 6/7] [D adv: -0.697360, aux: 0.001626] [G loss: -5.087693, adv: -5.359466, cls: 0.006000, cycle: 0.026577] ETA: 0:13:49.600715\n",
      "[Epoch 179/600] [Batch 6/7] [D adv: -0.677352, aux: 0.002669] [G loss: -5.400032, adv: -5.659439, cls: 0.004268, cycle: 0.025514] ETA: 0:13:47.021723\n",
      "[Epoch 180/600] [Batch 6/7] [D adv: -0.714385, aux: 0.001806] [G loss: -6.116278, adv: -6.368942, cls: 0.005863, cycle: 0.024680] ETA: 0:13:44.366843\n",
      "[Epoch 181/600] [Batch 6/7] [D adv: -0.655678, aux: 0.001659] [G loss: -5.809960, adv: -6.056548, cls: 0.005578, cycle: 0.024101] ETA: 0:13:41.807182\n",
      "[Epoch 182/600] [Batch 6/7] [D adv: -0.782124, aux: 0.001935] [G loss: -5.642007, adv: -5.890962, cls: 0.005818, cycle: 0.024314] ETA: 0:13:39.609602\n",
      "[Epoch 183/600] [Batch 6/7] [D adv: -0.685749, aux: 0.001471] [G loss: -5.607425, adv: -5.855973, cls: 0.006428, cycle: 0.024212] ETA: 0:13:37.083170\n",
      "[Epoch 184/600] [Batch 6/7] [D adv: -0.752096, aux: 0.002388] [G loss: -5.613904, adv: -5.852270, cls: 0.004381, cycle: 0.023399] ETA: 0:13:34.618765\n",
      "[Epoch 185/600] [Batch 6/7] [D adv: -0.665302, aux: 0.001179] [G loss: -6.465793, adv: -6.702528, cls: 0.003307, cycle: 0.023343] ETA: 0:13:32.106235\n",
      "[Epoch 186/600] [Batch 6/7] [D adv: -0.674550, aux: 0.001455] [G loss: -6.458226, adv: -6.693316, cls: 0.004459, cycle: 0.023063] ETA: 0:13:29.540714\n",
      "[Epoch 187/600] [Batch 6/7] [D adv: -0.825961, aux: 0.001516] [G loss: -6.122509, adv: -6.355844, cls: 0.003988, cycle: 0.022935] ETA: 0:13:27.106645\n",
      "[Epoch 188/600] [Batch 6/7] [D adv: -0.759238, aux: 0.001382] [G loss: -6.465110, adv: -6.697136, cls: 0.003989, cycle: 0.022804] ETA: 0:13:24.601995\n",
      "[Epoch 189/600] [Batch 6/7] [D adv: -0.739336, aux: 0.001266] [G loss: -6.501091, adv: -6.733338, cls: 0.002813, cycle: 0.022943] ETA: 0:13:22.109494\n",
      "[Epoch 190/600] [Batch 6/7] [D adv: -0.840434, aux: 0.001369] [G loss: -6.394139, adv: -6.623926, cls: 0.004197, cycle: 0.022559] ETA: 0:13:19.667457\n",
      "[Epoch 191/600] [Batch 6/7] [D adv: -0.812863, aux: 0.001727] [G loss: -5.786982, adv: -6.016382, cls: 0.004238, cycle: 0.022516] ETA: 0:13:17.193582\n",
      "[Epoch 192/600] [Batch 6/7] [D adv: -0.831975, aux: 0.001557] [G loss: -5.800909, adv: -6.036265, cls: 0.005895, cycle: 0.022946] ETA: 0:13:14.737742\n",
      "[Epoch 193/600] [Batch 6/7] [D adv: -0.754713, aux: 0.001564] [G loss: -5.961196, adv: -6.191898, cls: 0.003385, cycle: 0.022732] ETA: 0:13:12.252799\n",
      "[Epoch 194/600] [Batch 6/7] [D adv: -0.658657, aux: 0.001245] [G loss: -6.882141, adv: -7.107656, cls: 0.005900, cycle: 0.021962] ETA: 0:13:09.748606\n",
      "[Epoch 195/600] [Batch 6/7] [D adv: -0.957882, aux: 0.001167] [G loss: -6.252095, adv: -6.472018, cls: 0.003336, cycle: 0.021659] ETA: 0:13:07.305807\n",
      "[Epoch 196/600] [Batch 6/7] [D adv: -0.916238, aux: 0.001205] [G loss: -6.396078, adv: -6.616211, cls: 0.003491, cycle: 0.021664] ETA: 0:13:04.845795\n",
      "[Epoch 197/600] [Batch 6/7] [D adv: -0.877940, aux: 0.001482] [G loss: -5.763646, adv: -5.992343, cls: 0.007655, cycle: 0.022104] ETA: 0:13:02.488403\n",
      "[Epoch 198/600] [Batch 6/7] [D adv: -0.849447, aux: 0.001076] [G loss: -6.397598, adv: -6.619259, cls: 0.004756, cycle: 0.021691] ETA: 0:13:00.046178\n",
      "[Epoch 199/600] [Batch 6/7] [D adv: -0.827309, aux: 0.001226] [G loss: -5.846473, adv: -6.072168, cls: 0.004328, cycle: 0.022137] ETA: 0:12:57.643687\n",
      "[Epoch 200/600] [Batch 6/7] [D adv: -0.839750, aux: 0.001233] [G loss: -5.912085, adv: -6.132094, cls: 0.003852, cycle: 0.021616] ETA: 0:12:55.588341\n",
      "[Epoch 201/600] [Batch 6/7] [D adv: -0.771429, aux: 0.001130] [G loss: -7.010847, adv: -7.227845, cls: 0.002906, cycle: 0.021409] ETA: 0:12:53.247105\n",
      "[Epoch 202/600] [Batch 6/7] [D adv: -1.037747, aux: 0.000976] [G loss: -6.865239, adv: -7.079527, cls: 0.002475, cycle: 0.021181] ETA: 0:12:50.986241\n",
      "[Epoch 203/600] [Batch 6/7] [D adv: -0.932573, aux: 0.001551] [G loss: -5.626914, adv: -5.845505, cls: 0.006907, cycle: 0.021168] ETA: 0:12:48.725111\n",
      "[Epoch 204/600] [Batch 6/7] [D adv: -1.111851, aux: 0.001086] [G loss: -5.698190, adv: -5.917088, cls: 0.003889, cycle: 0.021501] ETA: 0:12:46.419554\n",
      "[Epoch 205/600] [Batch 6/7] [D adv: -0.700341, aux: 0.000958] [G loss: -6.974254, adv: -7.185950, cls: 0.005400, cycle: 0.020630] ETA: 0:12:44.050068\n",
      "[Epoch 206/600] [Batch 6/7] [D adv: -0.858517, aux: 0.001494] [G loss: -5.374489, adv: -5.598206, cls: 0.006579, cycle: 0.021714] ETA: 0:12:41.658741\n",
      "[Epoch 207/600] [Batch 6/7] [D adv: -1.037033, aux: 0.001095] [G loss: -5.133746, adv: -5.351758, cls: 0.004598, cycle: 0.021341] ETA: 0:12:39.272552\n",
      "[Epoch 208/600] [Batch 6/7] [D adv: -0.926530, aux: 0.000823] [G loss: -6.200496, adv: -6.415606, cls: 0.003227, cycle: 0.021188] ETA: 0:12:36.933964\n",
      "[Epoch 209/600] [Batch 6/7] [D adv: -1.002853, aux: 0.000823] [G loss: -6.321813, adv: -6.534226, cls: 0.003042, cycle: 0.020937] ETA: 0:12:34.628980\n",
      "[Epoch 210/600] [Batch 6/7] [D adv: -1.043430, aux: 0.000998] [G loss: -6.604601, adv: -6.814290, cls: 0.002693, cycle: 0.020700] ETA: 0:12:32.250546\n",
      "[Epoch 211/600] [Batch 6/7] [D adv: -1.080845, aux: 0.001011] [G loss: -5.876862, adv: -6.095103, cls: 0.004644, cycle: 0.021360] ETA: 0:12:29.922476\n",
      "[Epoch 212/600] [Batch 6/7] [D adv: -1.200771, aux: 0.000910] [G loss: -6.526626, adv: -6.737946, cls: 0.003334, cycle: 0.020799] ETA: 0:12:27.607299\n",
      "[Epoch 213/600] [Batch 6/7] [D adv: -0.890343, aux: 0.001243] [G loss: -6.243749, adv: -6.468923, cls: 0.004506, cycle: 0.022067] ETA: 0:12:25.238360\n",
      "[Epoch 214/600] [Batch 6/7] [D adv: -1.099563, aux: 0.001011] [G loss: -6.129929, adv: -6.343411, cls: 0.003158, cycle: 0.021032] ETA: 0:12:23.185476\n",
      "[Epoch 215/600] [Batch 6/7] [D adv: -0.925325, aux: 0.000869] [G loss: -7.096877, adv: -7.304296, cls: 0.002955, cycle: 0.020446] ETA: 0:12:20.886461\n",
      "[Epoch 216/600] [Batch 6/7] [D adv: -1.055828, aux: 0.000887] [G loss: -6.741299, adv: -6.951391, cls: 0.003292, cycle: 0.020680] ETA: 0:12:18.687881\n",
      "[Epoch 217/600] [Batch 6/7] [D adv: -1.226933, aux: 0.000923] [G loss: -5.909790, adv: -6.121161, cls: 0.004855, cycle: 0.020652] ETA: 0:12:16.397150\n",
      "[Epoch 218/600] [Batch 6/7] [D adv: -1.034139, aux: 0.000790] [G loss: -6.582299, adv: -6.787765, cls: 0.002606, cycle: 0.020286] ETA: 0:12:14.091730\n",
      "[Epoch 219/600] [Batch 6/7] [D adv: -1.123200, aux: 0.000724] [G loss: -6.356084, adv: -6.568993, cls: 0.002612, cycle: 0.021030] ETA: 0:12:11.766805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 220/600] [Batch 6/7] [D adv: -1.159294, aux: 0.001260] [G loss: -5.765652, adv: -5.980407, cls: 0.005003, cycle: 0.020975] ETA: 0:12:09.529324\n",
      "[Epoch 221/600] [Batch 6/7] [D adv: -0.740622, aux: 0.000807] [G loss: -7.543027, adv: -7.753865, cls: 0.001735, cycle: 0.020910] ETA: 0:12:07.216670\n",
      "[Epoch 222/600] [Batch 6/7] [D adv: -1.052465, aux: 0.001103] [G loss: -7.396464, adv: -7.603642, cls: 0.002493, cycle: 0.020468] ETA: 0:12:04.937350\n",
      "[Epoch 223/600] [Batch 6/7] [D adv: -1.072399, aux: 0.000894] [G loss: -6.612344, adv: -6.822404, cls: 0.004207, cycle: 0.020585] ETA: 0:12:02.682444\n",
      "[Epoch 224/600] [Batch 6/7] [D adv: -1.079119, aux: 0.001149] [G loss: -6.890839, adv: -7.109982, cls: 0.004950, cycle: 0.021419] ETA: 0:12:00.344710\n",
      "[Epoch 225/600] [Batch 6/7] [D adv: -0.691440, aux: 0.000995] [G loss: -8.413175, adv: -8.618754, cls: 0.003710, cycle: 0.020187] ETA: 0:11:58.063966\n",
      "[Epoch 226/600] [Batch 6/7] [D adv: -1.046289, aux: 0.000777] [G loss: -9.188096, adv: -9.400967, cls: 0.002015, cycle: 0.021086] ETA: 0:11:55.787951\n",
      "[Epoch 227/600] [Batch 6/7] [D adv: -1.256850, aux: 0.000912] [G loss: -7.869197, adv: -8.075331, cls: 0.004463, cycle: 0.020167] ETA: 0:11:53.478424\n",
      "[Epoch 228/600] [Batch 6/7] [D adv: -1.204396, aux: 0.000762] [G loss: -7.835314, adv: -8.048688, cls: 0.003699, cycle: 0.020967] ETA: 0:11:51.167837\n",
      "[Epoch 229/600] [Batch 6/7] [D adv: -0.988944, aux: 0.000820] [G loss: -8.894069, adv: -9.106711, cls: 0.002043, cycle: 0.021060] ETA: 0:11:48.881343\n",
      "[Epoch 230/600] [Batch 6/7] [D adv: -1.435679, aux: 0.000777] [G loss: -7.545870, adv: -7.758396, cls: 0.004915, cycle: 0.020761] ETA: 0:11:46.582500\n",
      "[Epoch 231/600] [Batch 6/7] [D adv: -0.904505, aux: 0.000834] [G loss: -8.862816, adv: -9.070986, cls: 0.002331, cycle: 0.020584] ETA: 0:11:44.284278\n",
      "[Epoch 232/600] [Batch 6/7] [D adv: -1.366161, aux: 0.000750] [G loss: -8.170027, adv: -8.386697, cls: 0.004523, cycle: 0.021215] ETA: 0:11:41.972330\n",
      "[Epoch 233/600] [Batch 6/7] [D adv: -1.277553, aux: 0.000738] [G loss: -8.222434, adv: -8.433317, cls: 0.003292, cycle: 0.020759] ETA: 0:11:39.670998\n",
      "[Epoch 234/600] [Batch 6/7] [D adv: -1.267129, aux: 0.001011] [G loss: -7.621878, adv: -7.841489, cls: 0.004987, cycle: 0.021462] ETA: 0:11:37.392588\n",
      "[Epoch 235/600] [Batch 6/7] [D adv: -1.140727, aux: 0.001004] [G loss: -7.498309, adv: -7.720018, cls: 0.005187, cycle: 0.021652] ETA: 0:11:35.126648\n",
      "[Epoch 236/600] [Batch 6/7] [D adv: -1.346615, aux: 0.000982] [G loss: -7.458657, adv: -7.674872, cls: 0.003834, cycle: 0.021238] ETA: 0:11:32.819592\n",
      "[Epoch 237/600] [Batch 6/7] [D adv: -1.149920, aux: 0.000871] [G loss: -8.444906, adv: -8.657695, cls: 0.002894, cycle: 0.020989] ETA: 0:11:30.898450\n",
      "[Epoch 238/600] [Batch 6/7] [D adv: -1.343246, aux: 0.000980] [G loss: -7.453353, adv: -7.675927, cls: 0.005655, cycle: 0.021692] ETA: 0:11:28.653491\n",
      "[Epoch 239/600] [Batch 6/7] [D adv: -1.328272, aux: 0.000748] [G loss: -8.483880, adv: -8.693340, cls: 0.004373, cycle: 0.020509] ETA: 0:11:26.419980\n",
      "[Epoch 240/600] [Batch 6/7] [D adv: -1.150768, aux: 0.000980] [G loss: -6.956487, adv: -7.179509, cls: 0.005873, cycle: 0.021715] ETA: 0:11:24.147439\n",
      "[Epoch 241/600] [Batch 6/7] [D adv: -1.132559, aux: 0.000962] [G loss: -6.348282, adv: -6.569271, cls: 0.008261, cycle: 0.021273] ETA: 0:11:21.962202\n",
      "[Epoch 242/600] [Batch 6/7] [D adv: -1.213851, aux: 0.001235] [G loss: -5.978860, adv: -6.206042, cls: 0.011848, cycle: 0.021533] ETA: 0:11:19.775616\n",
      "[Epoch 243/600] [Batch 6/7] [D adv: -0.938019, aux: 0.000907] [G loss: -7.983457, adv: -8.198131, cls: 0.002978, cycle: 0.021170] ETA: 0:11:17.575502\n",
      "[Epoch 244/600] [Batch 6/7] [D adv: -1.394437, aux: 0.000658] [G loss: -8.475321, adv: -8.690248, cls: 0.002174, cycle: 0.021275] ETA: 0:11:15.466318\n",
      "[Epoch 245/600] [Batch 6/7] [D adv: -1.378017, aux: 0.001127] [G loss: -7.516427, adv: -7.738545, cls: 0.006245, cycle: 0.021587] ETA: 0:11:13.279669\n",
      "[Epoch 246/600] [Batch 6/7] [D adv: -1.101973, aux: 0.000810] [G loss: -8.605045, adv: -8.819907, cls: 0.002213, cycle: 0.021265] ETA: 0:11:11.128051\n",
      "[Epoch 247/600] [Batch 6/7] [D adv: -1.413040, aux: 0.000752] [G loss: -7.798994, adv: -8.013291, cls: 0.003382, cycle: 0.021092] ETA: 0:11:09.013286\n",
      "[Epoch 248/600] [Batch 6/7] [D adv: -1.313594, aux: 0.000832] [G loss: -6.979709, adv: -7.210482, cls: 0.010041, cycle: 0.022073] ETA: 0:11:06.945262\n",
      "[Epoch 249/600] [Batch 6/7] [D adv: -1.416479, aux: 0.000843] [G loss: -7.909938, adv: -8.126199, cls: 0.003358, cycle: 0.021290] ETA: 0:11:04.845937\n",
      "[Epoch 250/600] [Batch 6/7] [D adv: -1.156240, aux: 0.000795] [G loss: -9.025055, adv: -9.233702, cls: 0.002164, cycle: 0.020648] ETA: 0:11:02.735284\n",
      "[Epoch 251/600] [Batch 6/7] [D adv: -1.446701, aux: 0.000526] [G loss: -9.175601, adv: -9.390804, cls: 0.002392, cycle: 0.021281] ETA: 0:11:00.567335\n",
      "[Epoch 252/600] [Batch 6/7] [D adv: -1.337304, aux: 0.000590] [G loss: -9.050799, adv: -9.264271, cls: 0.002790, cycle: 0.021068] ETA: 0:10:58.494008\n",
      "[Epoch 253/600] [Batch 6/7] [D adv: -1.381828, aux: 0.000619] [G loss: -8.894221, adv: -9.104495, cls: 0.003733, cycle: 0.020654] ETA: 0:10:56.447619\n",
      "[Epoch 254/600] [Batch 6/7] [D adv: -1.558244, aux: 0.000871] [G loss: -8.240870, adv: -8.454805, cls: 0.003235, cycle: 0.021070] ETA: 0:10:54.298481\n",
      "[Epoch 255/600] [Batch 6/7] [D adv: -1.307069, aux: 0.000995] [G loss: -7.775435, adv: -7.994404, cls: 0.004142, cycle: 0.021483] ETA: 0:10:52.131946\n",
      "[Epoch 256/600] [Batch 6/7] [D adv: -1.456316, aux: 0.000693] [G loss: -9.719796, adv: -9.931889, cls: 0.002457, cycle: 0.020964] ETA: 0:10:49.984361\n",
      "[Epoch 257/600] [Batch 6/7] [D adv: -1.574580, aux: 0.000663] [G loss: -8.888315, adv: -9.107578, cls: 0.002552, cycle: 0.021671] ETA: 0:10:47.883733\n",
      "[Epoch 258/600] [Batch 6/7] [D adv: -1.226477, aux: 0.000642] [G loss: -9.924053, adv: -10.133755, cls: 0.002718, cycle: 0.020698] ETA: 0:10:45.708143\n",
      "[Epoch 259/600] [Batch 6/7] [D adv: -1.565908, aux: 0.000733] [G loss: -9.720066, adv: -9.938965, cls: 0.003397, cycle: 0.021550] ETA: 0:10:43.707234\n",
      "[Epoch 260/600] [Batch 6/7] [D adv: -1.487772, aux: 0.000802] [G loss: -9.516153, adv: -9.728411, cls: 0.004632, cycle: 0.020763] ETA: 0:10:41.533351\n",
      "[Epoch 261/600] [Batch 6/7] [D adv: -1.503634, aux: 0.000679] [G loss: -9.740697, adv: -9.958740, cls: 0.002577, cycle: 0.021547] ETA: 0:10:39.392446\n",
      "[Epoch 262/600] [Batch 6/7] [D adv: -1.470400, aux: 0.000785] [G loss: -8.603523, adv: -8.826548, cls: 0.005512, cycle: 0.021751] ETA: 0:10:37.291650\n",
      "[Epoch 263/600] [Batch 6/7] [D adv: -1.332332, aux: 0.000553] [G loss: -9.865115, adv: -10.076699, cls: 0.002500, cycle: 0.020908] ETA: 0:10:35.186716\n",
      "[Epoch 264/600] [Batch 6/7] [D adv: -1.493865, aux: 0.000659] [G loss: -10.393538, adv: -10.605471, cls: 0.001882, cycle: 0.021005] ETA: 0:10:32.974856\n",
      "[Epoch 265/600] [Batch 6/7] [D adv: -1.472229, aux: 0.000740] [G loss: -8.609891, adv: -8.838564, cls: 0.007624, cycle: 0.022105] ETA: 0:10:30.842364\n",
      "[Epoch 266/600] [Batch 6/7] [D adv: -1.602058, aux: 0.000686] [G loss: -9.642598, adv: -9.862041, cls: 0.003063, cycle: 0.021638] ETA: 0:10:28.749774\n",
      "[Epoch 267/600] [Batch 6/7] [D adv: -1.506629, aux: 0.000907] [G loss: -8.509851, adv: -8.738017, cls: 0.011033, cycle: 0.021713] ETA: 0:10:26.634837\n",
      "[Epoch 268/600] [Batch 6/7] [D adv: -1.140888, aux: 0.000581] [G loss: -10.482810, adv: -10.696386, cls: 0.004635, cycle: 0.020894] ETA: 0:10:24.482194\n",
      "[Epoch 269/600] [Batch 6/7] [D adv: -1.527825, aux: 0.000671] [G loss: -10.505828, adv: -10.728745, cls: 0.001922, cycle: 0.022099] ETA: 0:10:22.390188\n",
      "[Epoch 270/600] [Batch 6/7] [D adv: -1.511093, aux: 0.000793] [G loss: -8.695154, adv: -8.918156, cls: 0.005855, cycle: 0.021715] ETA: 0:10:20.353372\n",
      "[Epoch 271/600] [Batch 6/7] [D adv: -1.509807, aux: 0.000701] [G loss: -9.225042, adv: -9.440404, cls: 0.003527, cycle: 0.021183] ETA: 0:10:18.252853\n",
      "[Epoch 272/600] [Batch 6/7] [D adv: -1.589052, aux: 0.000754] [G loss: -10.511921, adv: -10.720164, cls: 0.002274, cycle: 0.020597] ETA: 0:10:16.153208\n",
      "[Epoch 273/600] [Batch 6/7] [D adv: -1.428936, aux: 0.000726] [G loss: -11.526785, adv: -11.739369, cls: 0.001742, cycle: 0.021084] ETA: 0:10:14.094287\n",
      "[Epoch 274/600] [Batch 6/7] [D adv: -1.685128, aux: 0.000746] [G loss: -9.941780, adv: -10.162649, cls: 0.003865, cycle: 0.021700] ETA: 0:10:12.083510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 275/600] [Batch 6/7] [D adv: -1.665829, aux: 0.000665] [G loss: -9.789990, adv: -10.009172, cls: 0.005319, cycle: 0.021386] ETA: 0:10:10.175252\n",
      "[Epoch 276/600] [Batch 6/7] [D adv: -1.600109, aux: 0.000624] [G loss: -10.434029, adv: -10.638958, cls: 0.003762, cycle: 0.020117] ETA: 0:10:08.143343\n",
      "[Epoch 277/600] [Batch 6/7] [D adv: -1.599178, aux: 0.000590] [G loss: -8.990001, adv: -9.212851, cls: 0.003548, cycle: 0.021930] ETA: 0:10:06.074019\n",
      "[Epoch 278/600] [Batch 6/7] [D adv: -1.545178, aux: 0.000793] [G loss: -8.718011, adv: -8.940516, cls: 0.005327, cycle: 0.021718] ETA: 0:10:04.017651\n",
      "[Epoch 279/600] [Batch 6/7] [D adv: -1.718678, aux: 0.000644] [G loss: -10.206404, adv: -10.416114, cls: 0.003307, cycle: 0.020640] ETA: 0:10:01.963107\n",
      "[Epoch 280/600] [Batch 6/7] [D adv: -1.521677, aux: 0.000979] [G loss: -8.455217, adv: -8.687393, cls: 0.006599, cycle: 0.022558] ETA: 0:09:59.933460\n",
      "[Epoch 281/600] [Batch 6/7] [D adv: -1.807678, aux: 0.000646] [G loss: -9.747649, adv: -9.967605, cls: 0.003321, cycle: 0.021663] ETA: 0:09:57.861949\n",
      "[Epoch 282/600] [Batch 6/7] [D adv: -1.820505, aux: 0.000508] [G loss: -9.560435, adv: -9.779104, cls: 0.002846, cycle: 0.021582] ETA: 0:09:55.819336\n",
      "[Epoch 283/600] [Batch 6/7] [D adv: -1.773566, aux: 0.000558] [G loss: -8.865786, adv: -9.089104, cls: 0.004623, cycle: 0.021870] ETA: 0:09:53.774687\n",
      "[Epoch 284/600] [Batch 6/7] [D adv: -1.803652, aux: 0.000567] [G loss: -9.488504, adv: -9.699651, cls: 0.003129, cycle: 0.020802] ETA: 0:09:51.723909\n",
      "[Epoch 285/600] [Batch 6/7] [D adv: -1.705950, aux: 0.000613] [G loss: -8.881850, adv: -9.107623, cls: 0.005669, cycle: 0.022010] ETA: 0:09:49.715151\n",
      "[Epoch 286/600] [Batch 6/7] [D adv: -1.879334, aux: 0.000594] [G loss: -8.471042, adv: -8.697489, cls: 0.008010, cycle: 0.021844] ETA: 0:09:47.628140\n",
      "[Epoch 287/600] [Batch 6/7] [D adv: -1.274188, aux: 0.000617] [G loss: -10.483387, adv: -10.697087, cls: 0.002185, cycle: 0.021152] ETA: 0:09:45.589507\n",
      "[Epoch 288/600] [Batch 6/7] [D adv: -1.434626, aux: 0.000584] [G loss: -10.218366, adv: -10.441538, cls: 0.001454, cycle: 0.022172] ETA: 0:09:43.525323\n",
      "[Epoch 289/600] [Batch 6/7] [D adv: -1.610349, aux: 0.000595] [G loss: -8.729020, adv: -8.958628, cls: 0.012339, cycle: 0.021727] ETA: 0:09:41.469442\n",
      "[Epoch 290/600] [Batch 6/7] [D adv: -1.453028, aux: 0.001004] [G loss: -8.290514, adv: -8.522249, cls: 0.013227, cycle: 0.021851] ETA: 0:09:39.469621\n",
      "[Epoch 291/600] [Batch 6/7] [D adv: -1.540197, aux: 0.000621] [G loss: -10.908516, adv: -11.121701, cls: 0.002744, cycle: 0.021044] ETA: 0:09:37.473971\n",
      "[Epoch 292/600] [Batch 6/7] [D adv: -1.571670, aux: 0.000568] [G loss: -11.111296, adv: -11.344092, cls: 0.002878, cycle: 0.022992] ETA: 0:09:35.440242\n",
      "[Epoch 293/600] [Batch 6/7] [D adv: -1.422378, aux: 0.000883] [G loss: -11.220414, adv: -11.434122, cls: 0.002146, cycle: 0.021156] ETA: 0:09:33.412268\n",
      "[Epoch 294/600] [Batch 6/7] [D adv: -2.009153, aux: 0.000586] [G loss: -9.986998, adv: -10.209875, cls: 0.003005, cycle: 0.021987] ETA: 0:09:31.482205\n",
      "[Epoch 295/600] [Batch 6/7] [D adv: -1.873203, aux: 0.000622] [G loss: -9.081231, adv: -9.298351, cls: 0.006416, cycle: 0.021070] ETA: 0:09:29.483399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object BatchSampler.__iter__ at 0x7fe5f32ca1f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rom/miniconda3/envs/genex/lib/python3.10/site-packages/torch/utils/data/sampler.py\", line 248, in __iter__\n",
      "    yield batch\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_77004/320848527.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mG_AB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_77004/2576811038.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(scd, n_epochs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;31m# real images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mreal_validity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_B\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0;31m# Fake images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mfake_validity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_B\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/genex/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_77004/2576811038.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    148\u001b[0m           \u001b[0;31m#  data = torch.cat((data, label), -1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;31m#             atten = self.self_att(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;31m#             out = self.residual(out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# shortcut connection with attention embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/genex/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/genex/lib/python3.10/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/genex/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_77004/2578001821.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tabular_data)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#         return attn_output.squeeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/genex/lib/python3.10/site-packages/torch/fx/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# fallback to traceback.format_stack()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/genex/lib/python3.10/traceback.py\u001b[0m in \u001b[0;36mformat_list\u001b[0;34m(extracted_list)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mwhose\u001b[0m \u001b[0msource\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \"\"\"\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/genex/lib/python3.10/traceback.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             row.append('  File \"{}\", line {}, in {}\\n'.format(\n\u001b[0m\u001b[1;32m    441\u001b[0m                 frame.filename, frame.lineno, frame.name))\n\u001b[1;32m    442\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Stage II\n",
    "\n",
    "seed = 8\n",
    "n_epochs = 600\n",
    "\n",
    "adata.obs[\"Batch\"] = [int(each) for each in adata.obs[\"Batch\"]]\n",
    "# adata.obs[\"organ_tissue\"] = [int(each) for each in adata.obs[\"organ_tissue\"]]\n",
    "\n",
    "scd = ScDataset(adata)\n",
    "\n",
    "# output_results, scd = integrate_data(adata, key = batch_str, inc=True, n_epochs=5, n_batch = n_classes) #inc = True for multi-batch\n",
    "\n",
    "setup_seed(seed)\n",
    "\n",
    "print('Adata Info: ')\n",
    "print(adata)\n",
    "\n",
    "G_AB, D_B = train(scd, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ae5fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956779b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for saving good model\n",
    "\n",
    "# torch.save(G_AB.state_dict(), 'Generator_rom_pbmc.pth')\n",
    "# G_AB.load_state_dict(torch.load('Generator_rom_pancreatic.pth'))\n",
    "\n",
    "# torch.save(averager_D.averaged_model.state_dict(), 'Discriminator_rom_simulation_1.pth')\n",
    "# torch.save(D_B.state_dict(), 'Discriminator_rom_simulation_1.pth')\n",
    "# D_B.load_state_dict(torch.load('Discriminator_rom_pancreatic.pth'))\n",
    "\n",
    "\n",
    "# transfer to GPU for Making prediction\n",
    "# G_AB.cuda()\n",
    "# D_B.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4346f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "sc.settings.verbosity = 1\n",
    "\n",
    "sc.settings.set_figure_params(\n",
    "    dpi=200, frameon=False, figsize=(3, 3), facecolor='white')\n",
    "\n",
    "# new_groups = [str(each) for each in new_groups]\n",
    "new_batch = [str(each) for each in new_batch]\n",
    "\n",
    "# adata.X = sc.pp.normalize_per_cell(adata.X, copy = True, counts_per_cell_after=1e4, min_counts=0)\n",
    "# adata.X = sc.pp.log1p(adata.X)\n",
    "# # observer batch effect\n",
    "# sc.pp.scale(adata)\n",
    "# sc.pp.pca(adata)\n",
    "# sc.pp.neighbors(adata)\n",
    "\n",
    "# sc.tl.umap(adata)\n",
    "\n",
    "# sc.pl.umap(adata, color=['Batch', 'celltype'],\n",
    "#            palette=sc.pl.palettes.vega_20_scanpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e042db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define list to store weight\n",
    "# num_list = []\n",
    "\n",
    "# # compute for the average or median of mean and variance for gene level in each gene expression matrix\n",
    "# for i in range(len(batches)):\n",
    "#     temp_X = adata[adata.obs[\"batch\"] == batches[i]].X.toarray()\n",
    "#     num_list.append(len(temp_X))\n",
    "\n",
    "# # getting the weight array\n",
    "# weight_arr = np.array(num_list)\n",
    "# weight_arr = weight_arr / len(adata.X)\n",
    "\n",
    "# G_AB.eval()\n",
    "\n",
    "# # another weighted sum for gene expression\n",
    "# corrected_X = torch.zeros(scd.X.shape, device = \"cuda\")\n",
    "\n",
    "# for each in batches:\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         imgs = Variable(FloatTensor(scd.X))\n",
    "#         src_labels = Variable(FloatTensor(scd.labels))\n",
    "\n",
    "#         anchor_index_oh = enc_batch.transform(np.array([each]).reshape(-1, 1)).toarray()[0].tolist().index(1.0)\n",
    "#     #         celltypes = Variable(FloatTensor(scd.celltype))\n",
    "\n",
    "#     #         select batch for anchoring\n",
    "\n",
    "#         labels_ = [0.0]\n",
    "\n",
    "#         for each in range(c_dim - 1):\n",
    "#             labels_.append(0.0)\n",
    "\n",
    "#         labels_ = np.array(labels_)\n",
    "\n",
    "#         labels_ = np.tile(labels_, (scd.X.shape[0], 1))\n",
    "\n",
    "#         labels = Variable(FloatTensor(labels_))\n",
    "\n",
    "#         labels[:, anchor_index_oh] = 1.0\n",
    "#     #         labels[:, 3] = 1.0\n",
    "\n",
    "#     #         groups = Variable(FloatTensor(scd.groups))\n",
    "\n",
    "#         static_sample = G_AB(imgs, src_labels, labels)\n",
    "    \n",
    "#         # weighted sum based on the number of sample\n",
    "#         corrected_X = corrected_X + static_sample * weight_arr[anchor_index_oh]\n",
    "    \n",
    "# #     fake_data = static_sample.cpu().detach().numpy()  # pseudo-cell expression vector\n",
    "    \n",
    "# output_results = corrected_X.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36505fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     imgs = Variable(FloatTensor(scd.X))\n",
    "#     src_labels = Variable(FloatTensor(scd.labels))\n",
    "\n",
    "#     anchor_index_oh = enc_batch.transform(np.array([batch_anchor]).reshape(-1, 1)).toarray()[0].tolist().index(1.0)\n",
    "# #         celltypes = Variable(FloatTensor(scd.celltype))                                                  \n",
    "\n",
    "# #         select batch for anchoring\n",
    "\n",
    "#     labels_ = [0.0]\n",
    "\n",
    "#     for each in range(c_dim - 1):\n",
    "#         labels_.append(0.0)\n",
    "\n",
    "\n",
    "#     labels_ = np.array(labels_)\n",
    "\n",
    "#     labels_ = np.tile(labels_, (scd.X.shape[0], 1))\n",
    "\n",
    "#     labels = Variable(FloatTensor(labels_))\n",
    "\n",
    "#     labels[:, anchor_index_oh] = 1.0\n",
    "# #         labels[:, 3] = 1.0\n",
    "\n",
    "# #         groups = Variable(FloatTensor(scd.groups))\n",
    "\n",
    "#     static_sample = G_AB(imgs, src_labels, labels)\n",
    "    \n",
    "# #     fake_data = static_sample.cpu().detach().numpy()  # pseudo-cell expression vector\n",
    "\n",
    "# print(batch_anchor)\n",
    "    \n",
    "# output_results = static_sample.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d4d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_AB.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     imgs = Variable(FloatTensor(scd.X))\n",
    "#     src_labels = Variable(FloatTensor(scd.labels))\n",
    "    \n",
    "#     labels_ = [0.0]\n",
    "\n",
    "#     for each in range(c_dim - 1):\n",
    "#         labels_.append(0.0)\n",
    "\n",
    "#     labels_ = np.array(labels_)\n",
    "\n",
    "#     labels_ = np.tile(labels_, (scd.X.shape[0], 1))\n",
    "\n",
    "#     trg_labels = Variable(FloatTensor(labels_))\n",
    "    \n",
    "#     trg_labels[:, 0] = 0.0\n",
    "    \n",
    "#     sampled_c = Variable(FloatTensor(np.eye(c_dim)[np.random.choice(c_dim, imgs.size(0))]))\n",
    "    \n",
    "#     data = torch.cat((imgs, src_labels), -1)\n",
    "    \n",
    "#     latent_data = G_AB.encoder(data)\n",
    "    \n",
    "#     s_tilde, s_unrelated, s_related, s_related_tilde = G_AB.Style_Transformer(latent_data, trg_labels)\n",
    "# #     s_tilde, s_unrelated, s_related, s_related_tilde = G_AB.Style_Transformer(latent_data, sampled_c)\n",
    "    \n",
    "#     # get the mean and std for the trg embedding\n",
    "    \n",
    "#     gen_x_trg = latent_data + s_unrelated\n",
    "    \n",
    "#     gen_data = G_AB.decoder(gen_x_trg)\n",
    "\n",
    "#     static_sample = G_AB.relu(imgs + gen_data)\n",
    "    \n",
    "# output_results = static_sample.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae046f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_batches = sorted([int(each) for each in batches])\n",
    "temp_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b17702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define list to store weight\n",
    "# num_list = []\n",
    "\n",
    "# # compute for the average or median of mean and variance for gene level in each gene expression matrix\n",
    "# for i in range(len(temp_batches)):\n",
    "#     temp_X = adata[adata.obs[\"batch\"] == str(temp_batches[i])].X.toarray()\n",
    "\n",
    "#     num_list.append(len(temp_X))\n",
    "\n",
    "# # getting the weight array\n",
    "# weight_arr = np.array(num_list)\n",
    "\n",
    "# try:\n",
    "#     weight_arr = weight_arr / len(adata.X.toarray())\n",
    "\n",
    "# except:\n",
    "#     weight_arr = weight_arr / len(adata.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a52596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_AB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "averager_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f523868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "averager_G.averaged_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a2ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_AB.eval()\n",
    "\n",
    "# anchor_index_oh = enc_batch.transform(np.array([batches[anchor_index]]).reshape(-1, 1)).toarray()[0].tolist().index(1.0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    imgs = Variable(FloatTensor(scd.X))\n",
    "    src_labels = Variable(LongTensor(scd.labels))\n",
    "    src_groups = Variable(LongTensor(scd.groups))\n",
    "#     trg_labels = Variable(LongTensor(np.random.choice(c_dim, src_labels.size(0)).astype('long')))\n",
    "#     groups = Variable(LongTensor(scd.groups))\n",
    "\n",
    "    # torch.ones(imgs.size()[0]).long().cuda()\n",
    "    \n",
    "    static_sample = averager_G.averaged_model(imgs, src_labels, test = True)\n",
    "    static_sample =  F.relu(FloatTensor(scd.X + static_sample.cpu().detach().numpy())).cpu().detach().numpy()\n",
    "    \n",
    "#     labels_ = [0.0]\n",
    "\n",
    "#     for each in range(c_dim - 1):\n",
    "#         labels_.append(0.0)\n",
    "\n",
    "#     labels_ = np.array(labels_)\n",
    "\n",
    "#     labels_ = np.tile(labels_, (scd.X.shape[0], 1))\n",
    "# #     labels_ = np.tile(weight_arr * 3, (scd.X.shape[0], 1))\n",
    "# # \n",
    "#     trg_labels = Variable(LongTensor(labels_))\n",
    "    \n",
    "# #     trg_labels[:, anchor_index_oh] = 1.0\n",
    "    \n",
    "# #     trg_labels[:, 0] = 1.0\n",
    "    \n",
    "# #     sampled_c = Variable(FloatTensor(scd.random_target))\n",
    "    \n",
    "#     static_sample = G_AB(imgs, combine(trg_labels, groups)).cpu().detach().numpy()\n",
    "    \n",
    "#     # now scaling so that there is no negative values using minmaxscaler\n",
    "#     # better not to use scaling because some of the gene shall contain negative value.\n",
    "# #     max_val = np.amax(static_sample)\n",
    "    \n",
    "# #     scaler = MinMaxScaler((0, 1))\n",
    "\n",
    "# #     static_sample = scaler.fit_transform(static_sample)\n",
    "    \n",
    "output_results = static_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77a9e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_results = np.log1p(output_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abebd264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_results = adata.X\n",
    "\n",
    "max_val = np.amax(output_results)\n",
    "min_val = np.amin(output_results)\n",
    "\n",
    "print(\"Upper bound of the gene expression: \", max_val)\n",
    "print(\"Lower bound of the gene expression: \", min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde59273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_results = output_results * 10000\n",
    "# output_results = np.log1p(output_results)\n",
    "# output_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926a5a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "adata.obsm[\"X_latent\"] = output_results.copy()\n",
    "\n",
    "# output_results_norm = sc.pp.normalize_per_cell(output_results, counts_per_cell_after = 1e4, copy = True)\n",
    "# output_results_log_norm = sc.pp.log1p(output_results_norm)\n",
    "# output_results_scaled = sc.pp.scale(output_results_log_norm)\n",
    "# output_results_scaled = sc.pp.scale(output_results, copy = True)\n",
    "\n",
    "# adata.obsm[\"X_latent_norm\"] = output_results_log_norm\n",
    "# adata.obsm[\"X_latent_scaled\"] = output_results_scaled.copy()\n",
    "\n",
    "pca = PCA(n_components = 50, random_state = 8)\n",
    "\n",
    "# pca.fit(sc.pp.scale(adata.obsm[\"X_latent\"], copy = True))\n",
    "\n",
    "# X_pca = pca.transform(sc.pp.scale(adata.obsm[\"X_latent\"], copy = True))\n",
    "\n",
    "pca.fit(output_results)\n",
    "\n",
    "X_pca = pca.transform(output_results)\n",
    "\n",
    "adata.obsm[\"X_pca\"] = X_pca.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3923de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[\"Batch\"] = [str(each) for each in adata.obs[\"Batch\"]]\n",
    "# adata.obs[\"organ_tissue\"] = [str(each) for each in adata.obs[\"organ_tissue\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712b912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata, use_rep='X_pca', n_neighbors=30)\n",
    "# sc.external.pp.bbknn(adata_all, batch_key='batch', use_rep='X_latent')\n",
    "sc.tl.umap(adata, method = \"rapids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ae278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata.obsm[\"X_latent\"] = output_results\n",
    "\n",
    "# sc.pp.neighbors(adata, use_rep='X_latent', n_neighbors=30)\n",
    "# sc.external.pp.bbknn(adata, batch_key='batch', use_rep='X_latent')\n",
    "# sc.tl.umap(adata)\n",
    "sc.pl.umap(adata, color=[\"Batch\",\"Group_num\"],\n",
    "           palette=sc.pl.palettes.vega_20_scanpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94285e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.pl.umap(adata, color=[\"Method\",\"CellType\"],\n",
    "#            palette=sc.pl.palettes.vega_20_scanpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a8a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata, color=[\"Batch\"],\n",
    "           palette=sc.pl.palettes.vega_20_scanpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0dd65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata, color=[\"Group_num\"],\n",
    "           palette=sc.pl.palettes.vega_20_scanpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbabe7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c1d959",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_ari(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e1b745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute for the percentage of the condition\n",
    "\n",
    "group_1_per = (adata.obs[\"Group\"] == 0).sum() / len(adata)\n",
    "group_2_per = (adata.obs[\"Group\"] == 1).sum() / len(adata)\n",
    "percent_zeros = (adata.X == 0).mean()\n",
    "\n",
    "print(\"Group 1 percentage: %s\", group_1_per)\n",
    "print(\"Group 2 percentage: %s\", group_2_per)\n",
    "print(\"Zeros percentage: %s\", percent_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9680858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "dataset = \"simul11_dropout_47_b1_1000_b2_2000\"\n",
    "\n",
    "# true_down_genes_df = pd.read_csv('%s/true_down_genes.txt' % (dataset), sep=\"\\t\")\n",
    "# true_up_genes_df = pd.read_csv('%s/true_up_genes.txt' % (dataset), sep=\"\\t\")\n",
    "true_counts = pd.read_csv('%s/counts.txt' % (dataset), sep=\"\\t\").values.transpose()\n",
    "\n",
    "true_counts = true_counts.astype('float')\n",
    "\n",
    "true_counts = sc.pp.normalize_per_cell(true_counts, counts_per_cell_after=1e4, copy = True)\n",
    "true_counts = np.log1p(true_counts)\n",
    "\n",
    "subset_index = [int(each[4:]) - 1 for each in adata.obs.index.values]\n",
    "subset_genes = [int(each[4:]) - 1 for each in adata.var.index.values]\n",
    "true_counts = true_counts[subset_index]\n",
    "true_counts = true_counts[:, subset_genes]\n",
    "true_counts = true_counts.flatten()\n",
    "\n",
    "raw_data = adata.layers[\"X_raw\"].copy().flatten()\n",
    "corrected_data = adata.obsm[\"X_latent\"].copy().flatten()\n",
    "\n",
    "# for scanorama\n",
    "# corrected_data = adata_int.X.toarray().copy().flatten()\n",
    "\n",
    "wasserstein_distance(corrected_data, true_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b37699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "mean_squared_error(true_counts, corrected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "136f13f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata.write_h5ad(\"rom_model_pbmc(updated).h5ad\")\n",
    "# adata.write_h5ad(\"genex_simulation_2.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8456a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear GPU\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "febbc563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# X_raw = adata.X\n",
    "\n",
    "# pca = PCA(n_components = 50, random_state = 8)\n",
    "\n",
    "# pca.fit(X_raw)\n",
    "\n",
    "# X_pca_raw = pca.transform(X_raw)\n",
    "\n",
    "# adata.obsm[\"X_pca_raw\"] = X_pca_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "965c3c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.pp.neighbors(adata, use_rep='X_pca_raw', n_neighbors=30)\n",
    "# # sc.external.pp.bbknn(adata_all, batch_key='batch', use_rep='X_latent')\n",
    "# sc.tl.umap(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4642ca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # custom_palette = cc.glasbey_category10\n",
    "\n",
    "# sc.pl.umap(adata, color=[\"Batch\"],\n",
    "#            palette=sc.pl.palettes.vega_20_scanpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "72781026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.pl.umap(adata, color=[\"Group_num\"],\n",
    "#            palette=sc.pl.palettes.vega_20_scanpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "88362980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.special\n",
    "# from sklearn.metrics.cluster import adjusted_rand_score\n",
    "\n",
    "\n",
    "# def ari(adata, group1, group2, implementation=\"sklearn\"):\n",
    "#     \"\"\"Adjusted Rand Index\n",
    "#     The function is symmetric, so group1 and group2 can be switched\n",
    "#     For single cell integration evaluation the comparison is between predicted cluster\n",
    "#     assignments and the ground truth (e.g. cell type)\n",
    "#     :param adata: anndata object\n",
    "#     :param group1: string of column in adata.obs containing labels\n",
    "#     :param group2: string of column in adata.obs containing labels\n",
    "#     :param implementation: if set to 'sklearn', uses sklearn's implementation,\n",
    "#         otherwise native implementation is taken\n",
    "#     \"\"\"\n",
    "\n",
    "#     group1 = adata.obs[group1].to_numpy()\n",
    "#     group2 = adata.obs[group2].to_numpy()\n",
    "\n",
    "#     if len(group1) != len(group2):\n",
    "#         raise ValueError(\n",
    "#             f\"different lengths in group1 ({len(group1)}) and group2 ({len(group2)})\"\n",
    "#         )\n",
    "\n",
    "#     return adjusted_rand_score(group1, group2)\n",
    "\n",
    "# def compute_ari(adata):\n",
    "\n",
    "#     resolutions = None\n",
    "\n",
    "#     if resolutions is None:\n",
    "#         n = 20\n",
    "#         resolutions = [2 * x / n for x in range(1, n + 1)]\n",
    "\n",
    "#     score_max = 0\n",
    "#     res_max = resolutions[0]\n",
    "#     clustering = None\n",
    "#     score_all = []\n",
    "#     use_rep = \"X_pca_raw\"\n",
    "#     cluster_key = \"louvain\"\n",
    "#     label_key = \"Group\"\n",
    "    \n",
    "#     sc.pp.neighbors(adata, use_rep=use_rep)\n",
    "\n",
    "#     for res in resolutions:\n",
    "#         sc.tl.louvain(adata, resolution=res, key_added=cluster_key)\n",
    "#         score = ari(adata, label_key, cluster_key)\n",
    "#         score_all.append(score)\n",
    "#         if score_max < score:\n",
    "#             score_max = score\n",
    "#             res_max = res\n",
    "#             clustering = adata.obs[cluster_key]\n",
    "\n",
    "#     return score_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "58903a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_ari(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a22c2ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "55416f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.pp.neighbors(adata, use_rep='X_latent', n_neighbors=30)\n",
    "# # sc.external.pp.bbknn(adata_all, batch_key='batch', use_rep='X_latent')\n",
    "# sc.tl.umap(adata)\n",
    "# sc.pl.umap(adata, color=['Method', 'CellType'],\n",
    "#            palette=sc.pl.palettes.vega_20_scanpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fbbd0b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata_ = adata[adata.obs[\"Method\"] == \"10X_5prime\"]\n",
    "\n",
    "# sc.pl.umap(adata_, color=['CellType'],\n",
    "#            palette=sc.pl.palettes.vega_20_scanpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2431d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata_ = adata[adata.obs[\"Method\"] == \"10X_3prime\"]\n",
    "\n",
    "# sc.pl.umap(adata_, color=['CellType'],\n",
    "#            palette=sc.pl.palettes.vega_20_scanpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6bafb380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrected_df = pd.DataFrame(adata.obsm[\"X_latent\"].transpose())\n",
    "# corrected_df.index = adata.var[\"gene_name\"]\n",
    "# corrected_df.columns = adata.obs.index\n",
    "\n",
    "# corrected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e61de0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrected_df.to_csv(\"rom_model_corrected_expr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0b876dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_cell_df_corrected = pd.DataFrame(corrected_df.loc[\"CD79A\"])\n",
    "# one_cell_df_corrected[\"CellType\"] = adata.obs[\"CellType\"]\n",
    "# # one_cell_df_corrected.to_csv(\"CD79A_corrected.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e5beadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_df = pd.DataFrame(adata.layers[\"log_norm\"].copy().transpose())\n",
    "# raw_df.index = adata.var[\"gene_name\"]\n",
    "# raw_df.columns = adata.obs.index\n",
    "\n",
    "# raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d8be205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_cell_df_raw = pd.DataFrame(raw_df.loc[\"CD79A\"])\n",
    "# one_cell_df_raw[\"CellType\"] = adata.obs[\"CellType\"]\n",
    "# # one_cell_df_raw.to_csv(\"CD79A_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "13255d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Define two 1D distributions as PyTorch tensors\n",
    "# distribution1 = torch.tensor([0.1, 0.2, 0.3, 0.4])\n",
    "# distribution2 = torch.tensor([0.4, 0.3, 0.2, 0.1])\n",
    "\n",
    "# # Sort the distributions in ascending order\n",
    "# distribution1_sorted, indices1 = torch.sort(distribution1)\n",
    "# distribution2_sorted, indices2 = torch.sort(distribution2)\n",
    "\n",
    "# # Compute the Wasserstein distance\n",
    "# wasserstein_distance = torch.abs(distribution1_sorted - distribution2_sorted).sum()\n",
    "\n",
    "# print(f\"Wasserstein Distance: {wasserstein_distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8a1e88bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# def swd_loss(data1, data2, n_slices=128):\n",
    "#     \"\"\"\n",
    "#     Custom Slice-Wasserstein Distance (SWD) loss for 1D data.\n",
    "\n",
    "#     Args:\n",
    "#         data1 (Tensor): 1D data tensor of shape (batch_size, sequence_length).\n",
    "#         data2 (Tensor): 1D data tensor of the same shape as data1.\n",
    "#         n_slices (int): Number of slices to divide the data into.\n",
    "\n",
    "#     Returns:\n",
    "#         Tensor: SWD-like loss value.\n",
    "#     \"\"\"\n",
    "#     batch_size, sequence_length = data1.shape\n",
    "\n",
    "#     # Calculate the slice size to ensure even slicing\n",
    "#     slice_size = sequence_length // n_slices\n",
    "\n",
    "#     # Create slices (patches) of data\n",
    "#     data1_slices = data1.unfold(1, slice_size, 1)\n",
    "#     data2_slices = data2.unfold(1, slice_size, 1)\n",
    "\n",
    "#     # Calculate the mean and standard deviation of each slice\n",
    "#     mean_data1 = torch.mean(data1_slices, dim=2)\n",
    "#     std_data1 = torch.std(data1_slices, dim=2)\n",
    "#     mean_data2 = torch.mean(data2_slices, dim=2)\n",
    "#     std_data2 = torch.std(data2_slices, dim=2)\n",
    "\n",
    "#     # Calculate Wasserstein distance-like term between means and standard deviations\n",
    "#     wasserstein_mean = torch.mean(torch.abs(mean_data1 - mean_data2))\n",
    "#     wasserstein_std = torch.mean(torch.abs(std_data1 - std_data2))\n",
    "\n",
    "#     # Combine the two Wasserstein-like terms into a single loss\n",
    "#     swd_like_loss = wasserstein_mean + wasserstein_std\n",
    "\n",
    "#     return swd_like_loss\n",
    "\n",
    "# # Example usage:\n",
    "# data1 = torch.randn(64, 100)  # Replace with your 1D data tensors\n",
    "# data2 = torch.randn(64, 100)\n",
    "# loss = swd_loss(data1, data2)\n",
    "# loss\n",
    "# # Perform gradient descent to minimize the loss during adversarial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "df3f6620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "# kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "# # input should be a distribution in the log space\n",
    "# input = F.log_softmax(torch.randn(3, 5, requires_grad=True), dim=1)\n",
    "# # Sample a batch of distributions. Usually this would come from the dataset\n",
    "# target = F.softmax(torch.rand(3, 5), dim=1)\n",
    "# output = kl_loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "18b58bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "28823521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d07fe223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the colorcet paletteasdasd\n",
    "# custom_palette = cc.glasbey_category10\n",
    "\n",
    "# ax = sc.pl.umap(adata, color='Batch_num', palette=custom_palette,\n",
    "#            frameon=False, legend_fontsize=3.5, show=False)\n",
    "\n",
    "# # Set the plot title with the desired font size\n",
    "# ax.set_title('Predicted Cell Type', fontsize=6)\n",
    "\n",
    "# # Adjust the legend font size\n",
    "# handles, labels = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "# # Adjust the size of the circles\n",
    "# for handle in handles:\n",
    "#     handle.set_sizes([12])\n",
    "#     handle.set_edgecolor('black')\n",
    "#     handle.set_linewidth(0.5)\n",
    "\n",
    "# # Move the legend to the right side of the plot\n",
    "# plt.legend(handles=handles, labels=labels, prop={'size': 5}, ncol = 1, \n",
    "#            loc='upper center', bbox_to_anchor=(0.5, -0.08))\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "43140564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# heatmap_data = np.random.rand(6, 6)  # Generates random values between 0 and 1\n",
    "\n",
    "# plt = sns.heatmap(heatmap_data, cmap = \"YlGn\", cbar = False, linewidth=.6, linecolor = \"lightgrey\")\n",
    "# plt.grid(False)\n",
    "# plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d457aad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genex",
   "language": "python",
   "name": "genex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
